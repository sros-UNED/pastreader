{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e544b320",
   "metadata": {},
   "source": [
    "## OCR\n",
    "\n",
    "Código principal para la rutina de:\n",
    "1. lectura recursiva del directorio\n",
    "2. lectura y extracción de texto para ficheros con texto\n",
    "3. lectura, detección de texto y reconocimiento de caracteres para ficheros con imágenes\n",
    "\n",
    "Q: How to solve memory problems?\n",
    "- nvidia-smi\n",
    "- Check PIDs for memory usage and kill -9 PID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07ed39d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "643fe715a4604f2e91cb84979e413c01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./dev/pdf/9284.pdf\n",
      "Performing OCR in page 1/1\n",
      "[{'file_path': './dev/pdf/9284.pdf', 'page_num': 0, 'paddle_CER': 23.4362, 'paddle_WER': 55.8989, 'doctr_CER': 11.3967, 'doctr_WER': 41.2921, 'surya_CER': 2.7849, 'surya_WER': 11.236, 'olmo_CER': 2.9991, 'olmo_WER': 7.8652, 'fixed_CER': 0.2999, 'fixed_WER': 1.1236}]\n",
      "./dev/pdf/8969.pdf\n",
      "Performing OCR in page 1/1\n",
      "[{'file_path': './dev/pdf/8969.pdf', 'page_num': 0, 'paddle_CER': 9.1306, 'paddle_WER': 30.4843, 'doctr_CER': 11.6076, 'doctr_WER': 34.7578, 'surya_CER': 1.0685, 'surya_WER': 5.1282, 'olmo_CER': 3.1083, 'olmo_WER': 6.2678, 'fixed_CER': 0.4371, 'fixed_WER': 1.4245}]\n",
      "./dev/pdf/9430.pdf\n",
      "Performing OCR in page 1/1\n",
      "[{'file_path': './dev/pdf/9430.pdf', 'page_num': 0, 'paddle_CER': 14.3305, 'paddle_WER': 38.8601, 'doctr_CER': 13.4796, 'doctr_WER': 38.342, 'surya_CER': 2.7765, 'surya_WER': 12.6943, 'olmo_CER': 4.5231, 'olmo_WER': 9.0674, 'fixed_CER': 0.3583, 'fixed_WER': 1.8135}]\n",
      "./dev/pdf/9131.pdf\n",
      "Performing OCR in page 1/1\n",
      "Error querying the remote LLM service.\n",
      "[{'file_path': './dev/pdf/9131.pdf', 'page_num': 0, 'paddle_CER': 14.5897, 'paddle_WER': 37.7841, 'doctr_CER': 15.2844, 'doctr_WER': 50.2841, 'surya_CER': 4.4724, 'surya_WER': 16.1932, 'olmo_CER': 6.1224, 'olmo_WER': 12.7841, 'fixed_CER': 100.0, 'fixed_WER': 100.0}]\n",
      "./dev/pdf/9400.pdf\n",
      "Performing OCR in page 1/1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 553\u001b[0m\n\u001b[1;32m    551\u001b[0m directory_ocr \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./dev/corrected_ocr/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    552\u001b[0m output_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument_analysis_results.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 553\u001b[0m process_directory(directory, directory_ocr)\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fix_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m(model)\n",
      "Cell \u001b[0;32mIn[1], line 543\u001b[0m, in \u001b[0;36mprocess_directory\u001b[0;34m(directory, directory_ocr)\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mprint\u001b[39m(file_path)\n\u001b[0;32m--> 543\u001b[0m text \u001b[38;5;241m=\u001b[39m read_pdf(file_path, ocr_path)\n\u001b[1;32m    544\u001b[0m text \u001b[38;5;241m=\u001b[39m postprocess(text)\n\u001b[1;32m    545\u001b[0m save_text_file(text, txt_file_path)\n",
      "Cell \u001b[0;32mIn[1], line 447\u001b[0m, in \u001b[0;36mread_pdf\u001b[0;34m(file_path, ocr_path)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPerforming OCR in page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage_num\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(doc)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    446\u001b[0m ocr_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 447\u001b[0m ocr_text, result \u001b[38;5;241m=\u001b[39m ocr_image(img, page_num, file_path, ocr_path)\n\u001b[1;32m    448\u001b[0m results\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[1;32m    449\u001b[0m text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ocr_text\n",
      "Cell \u001b[0;32mIn[1], line 401\u001b[0m, in \u001b[0;36mocr_image\u001b[0;34m(img, page_num, file_path, ocr_path)\u001b[0m\n\u001b[1;32m    399\u001b[0m paddl_text \u001b[38;5;241m=\u001b[39m postprocess(ocr_column(img))\n\u001b[1;32m    400\u001b[0m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpaddle_CER\u001b[39m\u001b[38;5;124m\"\u001b[39m], result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpaddle_WER\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m verify_text(paddl_text, ocr_path)\n\u001b[0;32m--> 401\u001b[0m doctr_text \u001b[38;5;241m=\u001b[39m postprocess(ocr_column_doctr(img))\n\u001b[1;32m    402\u001b[0m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoctr_CER\u001b[39m\u001b[38;5;124m\"\u001b[39m], result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoctr_WER\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m verify_text(doctr_text, ocr_path)\n\u001b[1;32m    403\u001b[0m surya_text \u001b[38;5;241m=\u001b[39m postprocess(ocr_column_surya(img))\n",
      "Cell \u001b[0;32mIn[1], line 275\u001b[0m, in \u001b[0;36mocr_column_doctr\u001b[0;34m(column_image)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;66;03m# Use the OCR model\u001b[39;00m\n\u001b[1;32m    274\u001b[0m model \u001b[38;5;241m=\u001b[39m ocr_predictor(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 275\u001b[0m result \u001b[38;5;241m=\u001b[39m model([column_image])\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result\u001b[38;5;241m.\u001b[39mpages[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mblocks) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/doctr/models/predictor/pytorch.py:136\u001b[0m, in \u001b[0;36mOCRPredictor.forward\u001b[0;34m(self, pages, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m     crop_orientations \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    132\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: orientation[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfidence\u001b[39m\u001b[38;5;124m\"\u001b[39m: orientation[\u001b[38;5;241m1\u001b[39m]} \u001b[38;5;28;01mfor\u001b[39;00m orientation \u001b[38;5;129;01min\u001b[39;00m _crop_orientations\n\u001b[1;32m    133\u001b[0m     ]\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Identify character sequences\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m word_preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreco_predictor([crop \u001b[38;5;28;01mfor\u001b[39;00m page_crops \u001b[38;5;129;01min\u001b[39;00m crops \u001b[38;5;28;01mfor\u001b[39;00m crop \u001b[38;5;129;01min\u001b[39;00m page_crops], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m crop_orientations:\n\u001b[1;32m    138\u001b[0m     crop_orientations \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfidence\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m} \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m word_preds]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/doctr/models/recognition/predictor/pytorch.py:77\u001b[0m, in \u001b[0;36mRecognitionPredictor.forward\u001b[0;34m(self, crops, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m _params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, processed_batches \u001b[38;5;241m=\u001b[39m set_device_and_dtype(\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, processed_batches, _params\u001b[38;5;241m.\u001b[39mdevice, _params\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m     76\u001b[0m )\n\u001b[0;32m---> 77\u001b[0m raw \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(batch, return_preds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreds\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m processed_batches]\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Process outputs\u001b[39;00m\n\u001b[1;32m     80\u001b[0m out \u001b[38;5;241m=\u001b[39m [charseq \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m raw \u001b[38;5;28;01mfor\u001b[39;00m charseq \u001b[38;5;129;01min\u001b[39;00m batch]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/doctr/models/recognition/crnn/pytorch.py:199\u001b[0m, in \u001b[0;36mCRNN.forward\u001b[0;34m(self, x, target, return_model_output, return_preds)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mand\u001b[39;00m target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNeed to provide labels during training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 199\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeat_extractor(x)\n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m# B x C x H x W --> B x C*H x W --> B x W x C*H\u001b[39;00m\n\u001b[1;32m    201\u001b[0m c, h, w \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], features\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m], features\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m3\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups\n\u001b[1;32m    551\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tqdm, os, re, requests, csv, gc, io\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import torch\n",
    "import fastwer\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "gc.collect()\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "fix_mode = 1\n",
    "show_skip = False\n",
    "debug_mode = False\n",
    "\n",
    "keys = [\n",
    "    \"file_path\", \"page_num\", \"paddle_CER\", \"paddle_WER\", \"doctr_CER\",\n",
    "    \"doctr_WER\", \"surya_CER\", \"surya_WER\", \"olmo_CER\", \"olmo_WER\", \n",
    "    \"fixed_CER\", \"fixed_WER\"\n",
    "]\n",
    "results = {key: None for key in keys}\n",
    "\n",
    "def save_csv_file(text, output_file):\n",
    "    if text != \"\":\n",
    "        with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow([text])\n",
    "\n",
    "def save_text_file(text, output_file):\n",
    "    if text != \"\":\n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            file.write(text)\n",
    "\n",
    "if fix_mode != 1:\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, StoppingCriteria, StoppingCriteriaList\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"microsoft/Phi-3.5-mini-instruct\", \n",
    "        device_map=\"cuda\",\n",
    "        #device_map=\"cpu\",\n",
    "        torch_dtype=\"auto\", \n",
    "        trust_remote_code=True, \n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\")\n",
    "\n",
    "def fix_text_local(prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an AI assistant specialized in text correction and OCR error fixing.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    generation_args = {\n",
    "        \"max_new_tokens\": 1000,\n",
    "        \"return_full_text\": False,\n",
    "        \"temperature\": 0.0,\n",
    "        \"do_sample\": False,\n",
    "    }\n",
    "    corrected_text = \"\"\n",
    "    try:\n",
    "        output = pipe(messages, **generation_args)\n",
    "        # Extract the corrected text up to the '===END===' marker\n",
    "        corrected_text = output[0]['generated_text'].split('===END===')[0].strip()\n",
    "    except torch.cuda.OutOfMemoryError:\n",
    "        print(\"CUDA out of memory. Attempting to free some memory and retry.\")\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return corrected_text\n",
    "\n",
    "import json\n",
    "from openai import AzureOpenAI\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "\n",
    "endpoint = os.getenv(\"ENDPOINT_URL\", \"https://open-ia-service.openai.azure.com/\")\n",
    "deployment = os.getenv(\"DEPLOYMENT_NAME\", \"gpt-4o\")\n",
    "#AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_API_KEY = os.environ.get(\"AZURE_OPENAI_API_KEY\")\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key = AZURE_OPENAI_API_KEY,\n",
    "    api_version = \"2024-05-01-preview\",\n",
    "    azure_endpoint = endpoint \n",
    ")\n",
    "\n",
    "def fix_text_remote(prompt):\n",
    "    corrected_text = \"\"\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=deployment ,\n",
    "            messages= [\n",
    "            {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": prompt\n",
    "            }],\n",
    "            max_tokens=4000,\n",
    "            temperature=0,\n",
    "            top_p=0.95,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            stop=None,\n",
    "            stream=False\n",
    "        )\n",
    "        data = json.loads(completion.model_dump_json())\n",
    "        corrected_text = data['choices'][0]['message']['content']\n",
    "        parts = corrected_text.split('===END===')\n",
    "        if len(parts) > 1:\n",
    "            return parts[0].strip()\n",
    "    except:\n",
    "        print(\"Error querying the remote LLM service.\")\n",
    "        torch.cuda.empty_cache()\n",
    "    return corrected_text\n",
    "\n",
    "def set_prompt2(text):\n",
    "    prompt = (\n",
    "        \"You are an expert in text correction and OCR error fixing. Your task is to combine and correct several OCR outputs of the same text. \"\n",
    "        f\"Here are the texts:\\n\\n{text}\"\n",
    "        \"Instructions:\\n\"\n",
    "        \"1. Combine the texts, correcting any OCR errors.\\n\"\n",
    "        \"2. Provide only the corrected text, without any additional commentary.\\n\"\n",
    "        \"3. Maintain the original structure and formatting.\\n\"\n",
    "        \"4. Do not add any new information or explanations.\\n\"\n",
    "        \"5. Keep the text in its original language.\\n\"\n",
    "        \"6. Focus on fixing spelling, accents, and obvious OCR mistakes.\\n\"\n",
    "        \"7. End your response with '===END===' on a new line.\\n\\n\"\n",
    "        \"Corrected text:\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def set_prompt(text):\n",
    "    prompt = (\n",
    "        \"You are an expert in text correction and OCR error fixing. Your task is to combine and correct several OCR outputs of the same text. \"\n",
    "        f\"Here are the texts:\\n\\n{text}\"\n",
    "        \"Instructions:\\n\"\n",
    "        \"1. Combine the texts, correcting any OCR errors.\\n\"\n",
    "        \"2. Provide only the corrected text, without any additional commentary.\\n\"\n",
    "        \"3. Maintain the original structure and formatting.\\n\"\n",
    "        \"4. Do not add any new information or explanations.\\n\"\n",
    "        \"5. Join any words that have been separated by a hyphen at the end of a line. If there're blank spaces after the hyphen, remove them so the two parts of the word get joined correctly.\\n\"\n",
    "        \"6. The text is written using archaic Spanish spelling.\\n\"\n",
    "#\"7. Focus on fixing spelling and obvious OCR mistakes. but preserve all accent marks (ex. dió, á, fué) and special characters in words.\\n\"\n",
    "        \"7. Maintain all diacritical marks, old-fashioned spellings, and historical punctuation, such as the use of 'fué' instead of 'fue', 'dió' instead of 'dio', 'ví' instead of 'vi', 'á' instead of 'a' in prepositions. Do not replace older words or grammatical structures with modern equivalents.\\n\"\n",
    "#\"7. Preserve all accent marks (specially in words like 'dió', 'á', 'fué', 'ví') and special characters in words.\\n\"\n",
    "        \"8. Ensure that all words retain their original diacritics, such as accents (é, á, ó), tildes (ñ), and umlauts (ü), without alteration.\\n\"\n",
    "        \"9. Focus on fixing spelling and obvious OCR mistakes.\\n\"\n",
    "        \"10. End your response with '===END===' on a new line.\\n\\n\"\n",
    "        \"Corrected text:\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def fix_text(text):\n",
    "    prompt = set_prompt(text)\n",
    "    if fix_mode==0:\n",
    "        corrected_text = fix_text_local(prompt)\n",
    "    elif fix_mode==1:\n",
    "        corrected_text = fix_text_remote(prompt)\n",
    "    elif fix_mode==2:\n",
    "        corrected_text = fix_text_local(prompt)\n",
    "        text += f\"\\nText\\n{corrected_text}\"\n",
    "        prompt = set_prompt(text)\n",
    "        corrected_text = fix_text_remote(prompt)\n",
    "    else:\n",
    "        corrected_text = \"Error fixing the text\"\n",
    "    return corrected_text\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from paddleocr import PaddleOCR\n",
    "import numpy as np\n",
    "import cv2, fitz\n",
    "\n",
    "ocr = PaddleOCR(show_log=False, use_angle_cls=True, lang='es', use_gpu=False)\n",
    "\n",
    "def detectar_columnas(img):\n",
    "    if img is None:\n",
    "        print(\"Error: No se pudo procesar la imagen\")\n",
    "        return None, None\n",
    "\n",
    "    # Aplicar un umbral binario\n",
    "    _, thresh = cv2.threshold(img, 150, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "    # Encuentra los contornos para detectar las columnas\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Crear una máscara en blanco con el mismo tipo de dato\n",
    "    mask = np.zeros_like(img, dtype=np.uint8)\n",
    "\n",
    "    # Dibuja los contornos en la máscara\n",
    "    cv2.drawContours(mask, contours, -1, 255, -1)\n",
    "\n",
    "    # Proyectar la imagen a lo largo del eje x para encontrar espacios en blanco\n",
    "    vertical_projection = np.sum(mask, axis=0)\n",
    "\n",
    "    # Encuentra las áreas con bajos valores de proyección (posibles espacios entre columnas)\n",
    "    column_separators = np.where(vertical_projection < np.max(vertical_projection) * 0.1)[0]\n",
    "\n",
    "    if len(column_separators) > 1:\n",
    "        # Si se detecta un espacio entre columnas, divide la imagen en dos columnas\n",
    "        middle_separator = (column_separators[0] + column_separators[-1]) // 2\n",
    "        left_column = img[:, :middle_separator]\n",
    "        right_column = img[:, middle_separator:]\n",
    "        return left_column, right_column\n",
    "    else:\n",
    "        return img, None\n",
    "\n",
    "def ocr_full(img):\n",
    "    # Realiza la detección de texto y OCR\n",
    "    result = ocr.ocr(img, cls=True)\n",
    "\n",
    "    columna_izquierda = []\n",
    "    columna_derecha = []\n",
    "\n",
    "    if result is None or len(result) == 0:\n",
    "        print(\"No text detected in the image.\")\n",
    "    return \"\", \"\"\n",
    "\n",
    "    # Itera sobre las cajas detectadas\n",
    "    for res in result:\n",
    "        if res is None:\n",
    "            continue\n",
    "        for line in res:\n",
    "            box = line[0]\n",
    "\n",
    "            # Extrae los valores X de las coordenadas de la caja, asegurándose de que sean numéricos\n",
    "            x_coords = [point[0] for point in box]\n",
    "\n",
    "            # Calcula el centro de la caja en el eje X\n",
    "            if len(x_coords) > 0:\n",
    "                centro_x = np.mean(x_coords)\n",
    "                # Clasifica la caja en columna izquierda o derecha según el centro X\n",
    "                img_width = img.shape[1]\n",
    "                if centro_x < img_width / 2:\n",
    "                    columna_izquierda.append(line[1][0])  # El texto está en line[1][0]\n",
    "                else:\n",
    "                    columna_derecha.append(line[1][0])\n",
    "\n",
    "    # Unir las palabras de cada columna\n",
    "    texto_columna_izquierda = ' '.join(columna_izquierda)\n",
    "    texto_columna_derecha = ' '.join(columna_derecha)\n",
    "    return texto_columna_izquierda, texto_columna_derecha\n",
    "    \n",
    "def ocr_column(column_image):\n",
    "    if column_image is not None:\n",
    "        # Convierte la imagen de columna de numpy a PIL Image\n",
    "        pil_img = Image.fromarray(column_image)\n",
    "\n",
    "        # Aplica OCR en la columna\n",
    "        result = ocr.ocr(np.array(pil_img))\n",
    "        # Extrae el texto\n",
    "        if result[0] is not None:\n",
    "            text = \" \".join([line[1][0] for line in result[0]])\n",
    "        else:\n",
    "            text = \"\"\n",
    "        return text\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "from doctr.models import ocr_predictor\n",
    "def ocr_column_doctr(column_image):\n",
    "    if column_image is not None:\n",
    "        column_image = np.ascontiguousarray(column_image)\n",
    "        \n",
    "        # If the image is grayscale (2D), convert it to 3-channel\n",
    "        if len(column_image.shape) == 2:\n",
    "            column_image = np.stack((column_image,) * 3, axis=-1)\n",
    "        elif len(column_image.shape) == 3 and column_image.shape[2] == 1:\n",
    "            column_image = np.repeat(column_image, 3, axis=2)\n",
    "\n",
    "        # Use the OCR model\n",
    "        model = ocr_predictor(pretrained=True)\n",
    "        result = model([column_image])\n",
    "        if len(result.pages[0].blocks) == 0:\n",
    "            return \"\"\n",
    "        text = result.render()\n",
    "        return text\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "from contextlib import redirect_stdout, redirect_stderr\n",
    "\n",
    "os.environ[\"TQDM_DISABLE\"] = \"1\"\n",
    "\n",
    "def ocr_column_surya(column_image):\n",
    "    text = \"\"\n",
    "    if column_image is not None:\n",
    "        column_image = Image.fromarray(column_image)\n",
    "        langs = [\"es\"]\n",
    "        null_stream = io.StringIO()\n",
    "        try:\n",
    "            with redirect_stdout(null_stream), redirect_stderr(null_stream):\n",
    "                from surya.recognition import RecognitionPredictor\n",
    "                from surya.detection import DetectionPredictor\n",
    "                # Create predictor instances\n",
    "                detection_predictor = DetectionPredictor()\n",
    "                recognition_predictor = RecognitionPredictor()\n",
    "                \n",
    "                # Run OCR using the new API\n",
    "                predictions = recognition_predictor([column_image], [langs], detection_predictor)\n",
    "                \n",
    "                if predictions and predictions[0] is not None:\n",
    "                    # Access text_lines as an attribute, not using get()\n",
    "                    text_lines = predictions[0].text_lines\n",
    "                    text = \" \".join([line.text for line in text_lines])\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            gc.collect()\n",
    "    return text\n",
    "\n",
    "import base64, urllib.request\n",
    "from transformers import AutoProcessor, Qwen2VLForConditionalGeneration\n",
    "\n",
    "from olmocr.data.renderpdf import render_pdf_to_base64png\n",
    "from olmocr.prompts import build_finetuning_prompt\n",
    "from olmocr.prompts.anchor import get_anchor_text\n",
    "\n",
    "# Initialize the model\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\"allenai/olmOCR-7B-0225-preview\", torch_dtype=torch.bfloat16).eval()\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def ocr_column_olmo(img_array):\n",
    "    # Build the prompt, using document metadata\n",
    "    #anchor_text = get_anchor_text(\"./dev/pdf/9284.pdf\", 1, pdf_engine=\"pdfreport\", target_length=4000)\n",
    "    anchor_text = \"\"\n",
    "    prompt = build_finetuning_prompt(anchor_text)\n",
    "    pil_img = Image.fromarray(img_array) if img_array.shape[-1] == 3 else Image.fromarray(img_array, mode=\"L\")\n",
    "\n",
    "    # Guardar la imagen en memoria como PNG\n",
    "    buffer = BytesIO()\n",
    "    pil_img.save(buffer, format=\"PNG\")\n",
    "    \n",
    "    # Convertir a base64\n",
    "    img_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
    "\n",
    "    # Build the full prompt\n",
    "    messages = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": prompt},\n",
    "                        {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{img_base64}\"}},\n",
    "                    ],\n",
    "                }\n",
    "            ]\n",
    "    \n",
    "    # Apply the chat template and processor\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    main_image = Image.open(BytesIO(base64.b64decode(img_base64)))\n",
    "    \n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=[main_image],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = {key: value.to(device) for (key, value) in inputs.items()}\n",
    "    \n",
    "    # Generate the output\n",
    "    output = model.generate(\n",
    "                **inputs,\n",
    "                temperature=0.8,\n",
    "                max_new_tokens=1000,\n",
    "                num_return_sequences=1,\n",
    "                do_sample=True,\n",
    "            )\n",
    "    \n",
    "    # Decode the output\n",
    "    prompt_length = inputs[\"input_ids\"].shape[1]\n",
    "    new_tokens = output[:, prompt_length:]\n",
    "    data = processor.tokenizer.batch_decode(\n",
    "        new_tokens, skip_special_tokens=True\n",
    "    )\n",
    "    json_str = data[0]\n",
    "    json_obj = json.loads(json_str)\n",
    "    text = json_obj[\"natural_text\"]\n",
    "    return text\n",
    "\n",
    "\n",
    "def ocr_image(img, page_num, file_path, ocr_path):\n",
    "    full_text = \"\"\n",
    "    paddl_text = \"\"\n",
    "    doctr_text = \"\"\n",
    "    surya_text = \"\"\n",
    "    olmo_text = \"\"\n",
    "\n",
    "    text_list = []\n",
    "    result = {key: None for key in keys}\n",
    "    result[\"file_path\"] = file_path\n",
    "    result[\"page_num\"] = page_num\n",
    "\n",
    "    img = np.array(img)\n",
    "    # Detectar columnas\n",
    "    paddl_text = postprocess(ocr_column(img))\n",
    "    result[\"paddle_CER\"], result[\"paddle_WER\"] = verify_text(paddl_text, ocr_path)\n",
    "    doctr_text = postprocess(ocr_column_doctr(img))\n",
    "    result[\"doctr_CER\"], result[\"doctr_WER\"] = verify_text(doctr_text, ocr_path)\n",
    "    surya_text = postprocess(ocr_column_surya(img))\n",
    "    result[\"surya_CER\"], result[\"surya_WER\"] = verify_text(surya_text, ocr_path)\n",
    "    olmo_text = postprocess(ocr_column_olmo(img))\n",
    "    result[\"olmo_CER\"], result[\"olmo_WER\"] = verify_text(olmo_text, ocr_path)\n",
    "    if paddl_text != \"\":\n",
    "        text_list.append(paddl_text)\n",
    "    if doctr_text != \"\":\n",
    "        text_list.append(doctr_text)\n",
    "    if surya_text != \"\":\n",
    "        text_list.append(surya_text)\n",
    "    #if olmo_text != \"\":\n",
    "    #    text_list.append(olmo_text)\n",
    "   \n",
    "    if len(text_list) > 0:\n",
    "        text = \"\\n\".join([f\"\\nText\\n{s}\" for s in text_list])\n",
    "        text_fixed = postprocess(fix_text(text))\n",
    "        result[\"fixed_CER\"], result[\"fixed_WER\"] = verify_text(text_fixed, ocr_path)\n",
    "        \n",
    "        if debug_mode:\n",
    "            full_text = f\"[page {page_num+1}]\\n[OCRs]\\n{text}\\n[Result]\\n{text_fixed}\\n\\n\"\n",
    "        else:\n",
    "            #full_text = f\"[page {page_num+1}]\\n{text_fixed}\\n\\n\"\n",
    "            full_text = text_fixed\n",
    "    else:\n",
    "       #full_text = f\"[page {page_num+1}]\\n[Página sin texto]\\n\\n\"\n",
    "        full_text = f\"[Página sin texto]\\n\\n\"\n",
    "    return full_text, result\n",
    "\n",
    "def read_pdf(file_path, ocr_path):\n",
    "    text = \"\"\n",
    "    result = {key: None for key in keys}\n",
    "    results = []\n",
    "\n",
    "    doc = fitz.open(file_path)\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        pix = page.get_pixmap()\n",
    "        img_data = pix.tobytes(\"png\")\n",
    "        img = Image.open(BytesIO(img_data))\n",
    "        # Convertir la imagen a escala de grises (mejora el OCR)\n",
    "        img = img.convert('L')\n",
    "        img.save(f\"page_{page_num}.png\")\n",
    "        print(f'Performing OCR in page {page_num+1}/{len(doc)}')\n",
    "        ocr_text = \"\"\n",
    "        ocr_text, result = ocr_image(img, page_num, file_path, ocr_path)\n",
    "        results.append(result)\n",
    "        text += ocr_text\n",
    "    print(results)\n",
    "    return text\n",
    "\n",
    "def verify_text(text, ocr_path):\n",
    "\n",
    "    def format_with_comma(number):\n",
    "        return f\"{number:.2f}\".replace('.', ',')\n",
    "       \n",
    "    def normalize_text(input_text):\n",
    "        # Eliminar última línea si contiene \"biblioteca nacional\"\n",
    "        input_text = re.sub(r'\\s*biblioteca\\s+nacional\\s+de\\s+españa\\s*$', '', input_text, flags=re.IGNORECASE)\n",
    "        lines = input_text.split('\\n')\n",
    "\n",
    "        if lines and len(lines) > 0:\n",
    "            # Usar expresión regular para eliminar cualquier secuencia de dígitos\n",
    "            lines[0] = re.sub(r'\\d+', '', lines[0])\n",
    "            # Only access the second line if it exists\n",
    "            if len(lines) > 1:\n",
    "                lines[1] = re.sub(r'\\d+', '', lines[1])\n",
    "       \n",
    "        # Volver a unir las líneas\n",
    "        text_clean = '\\n'.join(lines)\n",
    "\n",
    "        # Eliminar caracteres especiales: \", », «, -, —\n",
    "        text_clean = text_clean.replace('\"', '')\n",
    "        text_clean = text_clean.replace('»', '')\n",
    "        text_clean = text_clean.replace('«', '')\n",
    "        text_clean = text_clean.replace('-', '')\n",
    "        text_clean = text_clean.replace('—', '')\n",
    "        \n",
    "        # Normalizar espacios\n",
    "        # 2. Eliminar espacios al inicio y final de cada línea\n",
    "        text_clean = '\\n'.join(line.strip() for line in text_clean.split('\\n'))\n",
    "        # 3. Eliminar líneas en blanco consecutivas\n",
    "        #text_clean = re.sub(r'\\n\\s*\\n', '\\n\\n', text_clean)\n",
    "        text_clean = re.sub(r'\\n', ' ', text_clean)\n",
    "        # 1. Reemplazar múltiples espacios con uno solo\n",
    "        text_clean = re.sub(r' +', ' ', text_clean)\n",
    "        text_clean = re.sub(r' +\\.', '.', text_clean)\n",
    "       \n",
    "        return text_clean\n",
    "    \n",
    "    # Comprobar si existe el fichero\n",
    "    if not os.path.exists(ocr_path):\n",
    "        print(f\"Error: El archivo {ocr_path} no existe.\")\n",
    "        return\n",
    "    \n",
    "    # Cargar el contenido del fichero en la variable Ref\n",
    "    try:\n",
    "        with open(ocr_path, 'r', encoding='utf-8') as file:\n",
    "            Ref = file.read()\n",
    "        \n",
    "        # Normalizar ambos textos\n",
    "        normalized_text = normalize_text(text)\n",
    "        normalized_ref = normalize_text(Ref)\n",
    "        #print(\"[text]\\n\", normalized_text)\n",
    "        #print(\"[Ref]\\n\", normalized_ref)\n",
    "        \n",
    "        # Calcular métricas\n",
    "        cer = fastwer.score_sent(normalized_text, normalized_ref, char_level=True)\n",
    "        wer = fastwer.score_sent(normalized_text, normalized_ref)\n",
    "        \n",
    "        #print(f'CER:{format_with_comma(cer)} - WER:{format_with_comma(wer)}')\n",
    "        \n",
    "        # Devolver los valores para posible uso posterior\n",
    "        return cer, wer\n",
    "    except Exception as e:\n",
    "        print(f\"Error al leer el archivo: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "def postprocess(text):\n",
    "    pattern = r'(\\w+)-\\s+(\\w+)'\n",
    "    # Reemplaza con las dos partes unidas\n",
    "    text = re.sub(pattern, r'\\1\\2', text)\n",
    "    text = re.sub(r\"\\s(vio)\\s\", r\" vió \", text)\n",
    "   \n",
    "    return text\n",
    "\n",
    "def process_directory(directory, directory_ocr):\n",
    "    for file in os.scandir(directory):\n",
    "        if file.is_file() and file.name.lower().endswith('.pdf'):\n",
    "            file_path = file.path\n",
    "            base_name = os.path.splitext(file.name)[0]\n",
    "            txt_file_name = base_name + '_.txt'\n",
    "            txt_file_path = os.path.join(directory, txt_file_name)\n",
    "            ocr_name = base_name + '.txt'\n",
    "            ocr_path = os.path.join(directory_ocr, ocr_name)\n",
    "            \n",
    "            if os.path.exists(txt_file_path):\n",
    "                if show_skip:\n",
    "                    print(f\"Skipping {file_path} - _.txt file already exists\")\n",
    "                continue\n",
    "            print(file_path)\n",
    "            text = read_pdf(file_path, ocr_path)\n",
    "            text = postprocess(text)\n",
    "            save_text_file(text, txt_file_path)\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            gc.collect()\n",
    "\n",
    "directory = \"./dev/pdf/\"\n",
    "directory_ocr = \"./dev/corrected_ocr/\"\n",
    "output_file = \"document_analysis_results.csv\"\n",
    "process_directory(directory, directory_ocr)\n",
    "\n",
    "if fix_mode != 1:\n",
    "    del(model)\n",
    "    del(tokenizer)\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "gc.collect()\n",
    "\n",
    "print('Process finished!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdd0750-4359-4bae-8289-4a466540ed3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import doctr\n",
    "import paddleocr\n",
    "\n",
    "print(doctr.__version__)\n",
    "print(paddleocr.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9401d7eb-391b-4c79-8eb2-e18c2a15b7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48b711596fe549ff84304894ff52957b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Qwen2VLForConditionalGeneration(\n",
       "  (visual): Qwen2VisionTransformerPretrainedModel(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
       "    )\n",
       "    (rotary_pos_emb): VisionRotaryEmbedding()\n",
       "    (blocks): ModuleList(\n",
       "      (0-31): 32 x Qwen2VLVisionBlock(\n",
       "        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): VisionSdpaAttention(\n",
       "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (mlp): VisionMlp(\n",
       "          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (act): QuickGELUActivation()\n",
       "          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (merger): PatchMerger(\n",
       "      (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=5120, out_features=3584, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (model): Qwen2VLModel(\n",
       "    (embed_tokens): Embedding(152064, 3584)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2VLDecoderLayer(\n",
       "        (self_attn): Qwen2VLSdpaAttention(\n",
       "          (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
       "          (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "          (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
       "          (rotary_emb): Qwen2VLRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "          (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "          (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2VLRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import base64\n",
    "import urllib.request\n",
    "\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, Qwen2VLForConditionalGeneration\n",
    "\n",
    "from olmocr.data.renderpdf import render_pdf_to_base64png\n",
    "from olmocr.prompts import build_finetuning_prompt\n",
    "from olmocr.prompts.anchor import get_anchor_text\n",
    "\n",
    "# Initialize the model\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\"allenai/olmOCR-7B-0225-preview\", torch_dtype=torch.bfloat16).eval()\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fda9d041-e9f1-4772-ad25-615a1d96df30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is the image of one page of a document, as well as some raw textual content that was previously extracted for it. Just return the plain text representation of this document as if you were reading it naturally.\n",
      "Do not hallucinate.\n",
      "RAW_TEXT_START\n",
      "Page dimensions: 366.0x590.0\n",
      "[167x557]Kranciiico MaraVcr\n",
      "[167x557]SU\n",
      "[76x528]El Dr. Valdivieso, director del\n",
      "[76x528]Jurado Médico-Farmacéu­\n",
      "[62x518]tico,\n",
      "[62x518]iej'ó su trabajo no menos notable: c Condiciones en <jue\n",
      "[64x507]la Prensa medica española debe continuar agregada á la\n",
      "[63x496]internacional, siempre que la vida social de ésta tenga todas\n",
      "[64x485]las garantías de seriedad y progreso que deben exigirse á\n",
      "[63x474]estas concentraciones de intereses afines entre los diferentes\n",
      "[64x461]países ».\n",
      "[77x450]El Dr. Gómez de la Mata, Director de\n",
      "[77x450]Los Nucnos Reme­\n",
      "[63x439]dios,\n",
      "[63x439]dio lectura á su trabajo acerca de la «Ck>nveniencia de\n",
      "[64x428]crear una sección humanitaria aneja ó filial de la Asociación\n",
      "[64x417]de la I'rensa Médica Espartóla en caso de muerte de algún\n",
      "[64x406]asociado fallo de recursos y hacerle un decoroso entierro;\n",
      "[64x395]esto sólo en casos extraordinarios y á petición, dd>i(iamente\n",
      "[64x384]autorizada, de la familia >.\n",
      "[78x373]El último trabajo de que se dio cuenta á la Asamblea en\n",
      "[64x361]eslu se.sión fué una comunicación, notable como todos sus\n",
      "[65x350]trabajos, presenUida por el Dr. Pulido, Presidente del Cole­\n",
      "[64x339]gio Médico de Madrid, y cuyo lema era: «Donación de pe­\n",
      "[64x328]riódicos científicos profesionales á las corporaciones mé­\n",
      "[64x316]dicas «.\n",
      "[78x305].Sesiones del día 1'2.\n",
      "[78x305]— En este día hulx) necesidad de ce­\n",
      "[65x293]lebrar dos sesiones, debido al gran número de trabajos some­\n",
      "[65x281]tidos á la Asamblea.\n",
      "[78x269]En la sesión de la maflana fueron leídos los trabajos ti-\n",
      "[65x257]guienles:\n",
      "[79x246]1.0 «Conveniencia de un cambio mutuo de noticiM ad»\n",
      "[66x235]ministrativas confidenciales, para que todos s^an qui^MS\n",
      "[65x224]faltan habítualmentc\n",
      "[65x224]&\n",
      "[65x224]sus compromisos como suscriptores\n",
      "[65x213]ó anunciantes, bien negándose sistemáticamente\n",
      "[65x213]á\n",
      "[65x213]pagar sus\n",
      "[65x202]descubiertos ó solicitando el envío de libros ú objetos que\n",
      "[66x191]luego no pagan, cometiendo de este modo pequeflas estafas».\n",
      "[65x180]Ponente Sr. Remartlnez, Director de la\n",
      "[65x180]Veterinaria Española\n",
      "[78x169]2.0 «'Medios prácticos de estrechar los latos de unión\n",
      "[65x158]entre los periódicos y periodistas de las diversas ramas de\n",
      "[65x147]las ciencias médicas, para levantar el concepto de la prensa\n",
      "[65x136]y facilitar la mejor consecución de sus fines». Ponente,\n",
      "[66x125]Dr. Royo Villanova^ Director de\n",
      "[66x125]La Clínica Moderna,\n",
      "[66x125]de Za­\n",
      "[66x114]ragoza.\n",
      "[79x102]3.\" !«Resoluciones que procedan referentes á anuncios\n",
      "[65x91]que por su falta de seriedad, por tener dejos de charlatanismo\n",
      "[66x80]6 por oponerse á las más vulgares opiniones cientificas, no\n",
      "[66x69]deben aparecer en publicaciones radnctadas por hombres de\n",
      "[66x58]ciencia y desuñadas á ellos». Ponente, Dr. Chabás, Director\n",
      "[66x47]de\n",
      "[66x47]La Salud Pública,\n",
      "[66x47]de Valencia.\n",
      "[36x806]Kranciiico MaraVcr SU \\nEl Dr. Valdivieso, director del Jurado Médico-Farmacéu­\\ntico, iej'ó su trabajo no menos notable: ... por hombres de \\nciencia y desuñadas á ellos». Ponente, Dr. Chabás, Director \\nde La Salud Pública, de Valencia.\n",
      "\n",
      "RAW_TEXT_END\n",
      "['{\"primary_language\":\"es\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"El Dr. Valdivieso, director del Jurado Médico-Farmacéutico, leyó su trabajo no menos notable: «Condiciones en que la Prensa médica española debe continuar agregada a la internacional, siempre que la vida social de ésta tenga todas las garantías de seriedad y progreso que deben exigirse a estas concentraciones de intereses afines entre los diferentes países».\\\\n\\\\nEl Dr. Gómez de la Mata, Director de Los Nuevos Remedios, dio lectura a su trabajo acerca de la «Conveniencia de crear una sección humanitaria aneja ó filial de la Asociación de la Prensa Médica Española en caso de muerte de algún asociado falto de recursos y hacerle un decoroso entierro; esto sólo en casos extraordinarios y á petición, debidamente autorizada, de la familia».\\\\n\\\\nEl último trabajo de que se dio cuenta á la Asamblea en esta sesión fue una comunicación, notable como todos sus trabajos, presentada por el Dr. Pulido, Presidente del Colegio Médico de Madrid, y cuyo lema era: «Donación de periódicos científicos profesionales á las corporaciones médicas».\\\\n\\\\nSesiones del día 12. — En este día hubo necesidad de celebrar dos sesiones, debido al gran número de trabajos sometidos á la Asamblea.\\\\n\\\\nEn la sesión de la mañana fueron leídos los trabajos siguientes:\\\\n\\\\n1.º «Conveniencia de un cambio mutuo de noticias administrativas confidenciales, para que todos sepan quiénes faltan habitualmente á sus compromisos como suscriptores ó anunciantes, bien negándose sistemáticamente á pagar sus descubiertos ó solicitando el envío de libros ó objetos que luego no pagan, cometiendo de este modo pequeñas estafas». Ponente Sr. Remartínez, Director de la Veterinaria Española.\\\\n\\\\n2.º «Medios prácticos de estrechar los lazos de unión entre los periódicos y periodistas de las diversas ramas de las ciencias médicas, para levantar el concepto de la prensa y facilitar la mejor consecución de sus fines». Ponente, Dr. Royo Villanova, Director de La Clínica Moderna, de Zaragoza.\\\\n\\\\n3.º «Resoluciones que procedan referentes á anuncios que por su falta de seriedad, por tener deudos de charlatanismo ó por oponerse á las más vulgares opiniones científicas, no deben aparecer en publicaciones radicadas por hombres de ciencia y destinadas á ellos». Ponente, Dr. Chabás, Director de La Salud Pública, de Valencia.\"}']\n"
     ]
    }
   ],
   "source": [
    "# Render page 2 to an image\n",
    "image_base64 = render_pdf_to_base64png(\"./dev/pdf/9284.pdf\", 1, target_longest_image_dim=1024)\n",
    "\n",
    "# Build the prompt, using document metadata\n",
    "anchor_text = get_anchor_text(\"./dev/pdf/9284.pdf\", 1, pdf_engine=\"pdfreport\", target_length=4000)\n",
    "prompt = build_finetuning_prompt(anchor_text)\n",
    "print(prompt)\n",
    "# Build the full prompt\n",
    "messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{image_base64}\"}},\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "\n",
    "# Apply the chat template and processor\n",
    "text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "main_image = Image.open(BytesIO(base64.b64decode(image_base64)))\n",
    "\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=[main_image],\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = {key: value.to(device) for (key, value) in inputs.items()}\n",
    "\n",
    "# Generate the output\n",
    "output = model.generate(\n",
    "            **inputs,\n",
    "            temperature=0.8,\n",
    "            max_new_tokens=1000,\n",
    "            num_return_sequences=1,\n",
    "            do_sample=True,\n",
    "        )\n",
    "\n",
    "# Decode the output\n",
    "prompt_length = inputs[\"input_ids\"].shape[1]\n",
    "new_tokens = output[:, prompt_length:]\n",
    "text_output = processor.tokenizer.batch_decode(\n",
    "    new_tokens, skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(text_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

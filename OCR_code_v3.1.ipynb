{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e544b320",
   "metadata": {},
   "source": [
    "## OCR\n",
    "\n",
    "Código principal para la rutina de:\n",
    "1. lectura recursiva del directorio\n",
    "2. lectura y extracción de texto para ficheros con texto\n",
    "3. lectura, detección de texto y reconocimiento de caracteres para ficheros con imágenes\n",
    "\n",
    "Q: How to solve memory problems?\n",
    "- nvidia-smi\n",
    "- Check PIDs for memory usage and kill -9 PID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b355d867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pybind11\n",
      "  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
      "Using cached pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
      "Installing collected packages: pybind11\n",
      "Successfully installed pybind11-2.13.6\n",
      "Collecting fastwer\n",
      "  Using cached fastwer-0.1.3.tar.gz (4.6 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pybind11 in /Users/salvador/opt/anaconda3/envs/augur/lib/python3.12/site-packages (from fastwer) (2.13.6)\n",
      "Building wheels for collected packages: fastwer\n",
      "  Building wheel for fastwer (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fastwer: filename=fastwer-0.1.3-cp312-cp312-macosx_10_15_x86_64.whl size=59421 sha256=4fa510cce497c28cdfc2ed7f2f82253a68f36e6618f3be11afb67bd75eb3c501\n",
      "  Stored in directory: /Users/salvador/Library/Caches/pip/wheels/7b/9a/f7/3b890b9a62baace0b8838c02151d94c76fd47d87cdb9249490\n",
      "Successfully built fastwer\n",
      "Installing collected packages: fastwer\n",
      "Successfully installed fastwer-0.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install pybind11\n",
    "!pip install fastwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3a21a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm, os, re, requests, csv, gc, io\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import torch\n",
    "import fastwer\n",
    "import json\n",
    "from openai import AzureOpenAI\n",
    "#from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from paddleocr import PaddleOCR\n",
    "import numpy as np\n",
    "import cv2, fitz\n",
    "from contextlib import redirect_stdout, redirect_stderr\n",
    "from doctr.models import ocr_predictor\n",
    "import base64, urllib.request\n",
    "from transformers import AutoProcessor, Qwen2VLForConditionalGeneration\n",
    "\n",
    "from olmocr.data.renderpdf import render_pdf_to_base64png\n",
    "from olmocr.prompts import build_finetuning_prompt\n",
    "from olmocr.prompts.anchor import get_anchor_text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c348346",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "gc.collect()\n",
    "torch.random.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fac3355",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inicializacion\n",
    "fix_mode = 1\n",
    "show_skip = False\n",
    "debug_mode = False\n",
    "directory = \"./dev/pdf/\"\n",
    "directory_ocr = \"./dev/corrected_ocr/\"\n",
    "output_file = \"document_analysis_results.csv\"\n",
    "\n",
    "ocr = PaddleOCR(show_log=False, use_angle_cls=True, lang='es', use_gpu=False)\n",
    "endpoint = os.getenv(\"ENDPOINT_URL\", \"https://open-ia-service.openai.azure.com/\")\n",
    "deployment = os.getenv(\"DEPLOYMENT_NAME\", \"gpt-4o\")\n",
    "#AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_API_KEY = os.environ.get(\"AZURE_OPENAI_API_KEY\")\n",
    "client = AzureOpenAI(\n",
    "    api_key = AZURE_OPENAI_API_KEY,\n",
    "    api_version = \"2024-05-01-preview\",\n",
    "    azure_endpoint = endpoint \n",
    "os.environ[\"TQDM_DISABLE\"] = \"1\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\"allenai/olmOCR-7B-0225-preview\", torch_dtype=torch.bfloat16).eval()\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc4a665a",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [\n",
    "    \"file_path\", \"page_num\", \"paddle_CER\", \"paddle_WER\", \"doctr_CER\",\n",
    "    \"doctr_WER\", \"surya_CER\", \"surya_WER\", \"olmo_CER\", \"olmo_WER\", \n",
    "    \"fixed_CER\", \"fixed_WER\"\n",
    "]\n",
    "results = {key: None for key in keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c4f00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fix_mode != 1:\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, StoppingCriteria, StoppingCriteriaList\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"microsoft/Phi-3.5-mini-instruct\", \n",
    "        device_map=\"cuda\",\n",
    "        #device_map=\"cpu\",\n",
    "        torch_dtype=\"auto\", \n",
    "        trust_remote_code=True, \n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\")\n",
    "if fix_mode != 1:\n",
    "    del(model)\n",
    "    del(tokenizer)\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc15b1f",
   "metadata": {},
   "source": [
    "## Funciones de salvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e1b3e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funciones de salvar\n",
    "def save_csv_file(text, output_file):\n",
    "    if text != \"\":\n",
    "        with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow([text])\n",
    "\n",
    "def save_text_file(text, output_file):\n",
    "    if text != \"\":\n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            file.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca31d6b",
   "metadata": {},
   "source": [
    "## Funciones OCR y Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ed39d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fix_text_local(prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an AI assistant specialized in text correction and OCR error fixing.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    generation_args = {\n",
    "        \"max_new_tokens\": 1000,\n",
    "        \"return_full_text\": False,\n",
    "        \"temperature\": 0.0,\n",
    "        \"do_sample\": False,\n",
    "    }\n",
    "    corrected_text = \"\"\n",
    "    try:\n",
    "        output = pipe(messages, **generation_args)\n",
    "        # Extract the corrected text up to the '===END===' marker\n",
    "        corrected_text = output[0]['generated_text'].split('===END===')[0].strip()\n",
    "    except torch.cuda.OutOfMemoryError:\n",
    "        print(\"CUDA out of memory. Attempting to free some memory and retry.\")\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return corrected_text\n",
    "\n",
    "\n",
    "def fix_text_remote(prompt):\n",
    "    corrected_text = \"\"\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=deployment,\n",
    "            messages= [\n",
    "            {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": prompt\n",
    "            }],\n",
    "            max_tokens=4000,\n",
    "            temperature=0,\n",
    "            top_p=0.95, #OJO\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            stop=None,\n",
    "            stream=False\n",
    "        )\n",
    "        data = json.loads(completion.model_dump_json())\n",
    "        corrected_text = data['choices'][0]['message']['content']\n",
    "        parts = corrected_text.split('===END===')\n",
    "        if len(parts) > 1:\n",
    "            return parts[0].strip()\n",
    "    except:\n",
    "        print(\"Error querying the remote LLM service.\")\n",
    "        torch.cuda.empty_cache()\n",
    "    return corrected_text\n",
    "# Prompts\n",
    "def set_prompt2(text):\n",
    "    prompt = (\n",
    "        \"You are an expert in text correction and OCR error fixing. Your task is to combine and correct several OCR outputs of the same text. \"\n",
    "        f\"Here are the texts:\\n\\n{text}\"\n",
    "        \"Instructions:\\n\"\n",
    "        \"1. Combine the texts, correcting any OCR errors.\\n\"\n",
    "        \"2. Provide only the corrected text, without any additional commentary.\\n\"\n",
    "        \"3. Maintain the original structure and formatting.\\n\"\n",
    "        \"4. Do not add any new information or explanations.\\n\"\n",
    "        \"5. Keep the text in its original language.\\n\"\n",
    "        \"6. Focus on fixing spelling, accents, and obvious OCR mistakes.\\n\"\n",
    "        \"7. End your response with '===END===' on a new line.\\n\\n\"\n",
    "        \"Corrected text:\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def set_prompt(text):\n",
    "    prompt = (\n",
    "        \"You are an expert in text correction and OCR error fixing. Your task is to combine and correct several OCR outputs of the same text. \"\n",
    "        f\"Here are the texts:\\n\\n{text}\"\n",
    "        \"Instructions:\\n\"\n",
    "        \"1. Combine the texts, correcting any OCR errors.\\n\"\n",
    "        \"2. Provide only the corrected text, without any additional commentary.\\n\"\n",
    "        \"3. Maintain the original structure and formatting.\\n\"\n",
    "        \"4. Do not add any new information or explanations.\\n\"\n",
    "        \"5. Join any words that have been separated by a hyphen at the end of a line. If there're blank spaces after the hyphen, remove them so the two parts of the word get joined correctly.\\n\"\n",
    "        \"6. The text is written using archaic Spanish spelling.\\n\"\n",
    "#\"7. Focus on fixing spelling and obvious OCR mistakes. but preserve all accent marks (ex. dió, á, fué) and special characters in words.\\n\"\n",
    "        \"7. Maintain all diacritical marks, old-fashioned spellings, and historical punctuation, such as the use of 'fué' instead of 'fue', 'dió' instead of 'dio', 'ví' instead of 'vi', 'á' instead of 'a' in prepositions. Do not replace older words or grammatical structures with modern equivalents.\\n\"\n",
    "#\"7. Preserve all accent marks (specially in words like 'dió', 'á', 'fué', 'ví') and special characters in words.\\n\"\n",
    "        \"8. Ensure that all words retain their original diacritics, such as accents (é, á, ó), tildes (ñ), and umlauts (ü), without alteration.\\n\"\n",
    "        \"9. Focus on fixing spelling and obvious OCR mistakes.\\n\"\n",
    "        \"10. End your response with '===END===' on a new line.\\n\\n\"\n",
    "        \"Corrected text:\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def fix_text(text):\n",
    "    prompt = set_prompt(text)\n",
    "    if fix_mode==0:\n",
    "        corrected_text = fix_text_local(prompt)\n",
    "    elif fix_mode==1:\n",
    "        corrected_text = fix_text_remote(prompt)\n",
    "    elif fix_mode==2:\n",
    "        corrected_text = fix_text_local(prompt)\n",
    "        text += f\"\\nText\\n{corrected_text}\"\n",
    "        prompt = set_prompt(text)\n",
    "        corrected_text = fix_text_remote(prompt)\n",
    "    else:\n",
    "        corrected_text = \"Error fixing the text\"\n",
    "    return corrected_text\n",
    "\n",
    "\n",
    "def detectar_columnas(img):\n",
    "    if img is None:\n",
    "        print(\"Error: No se pudo procesar la imagen\")\n",
    "        return None, None\n",
    "    # Aplicar un umbral binario\n",
    "    _, thresh = cv2.threshold(img, 150, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "    # Encuentra los contornos para detectar las columnas\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Crear una máscara en blanco con el mismo tipo de dato\n",
    "    mask = np.zeros_like(img, dtype=np.uint8)\n",
    "\n",
    "    # Dibuja los contornos en la máscara\n",
    "    cv2.drawContours(mask, contours, -1, 255, -1)\n",
    "\n",
    "    # Proyectar la imagen a lo largo del eje x para encontrar espacios en blanco\n",
    "    vertical_projection = np.sum(mask, axis=0)\n",
    "\n",
    "    # Encuentra las áreas con bajos valores de proyección (posibles espacios entre columnas)\n",
    "    column_separators = np.where(vertical_projection < np.max(vertical_projection) * 0.1)[0]\n",
    "\n",
    "    if len(column_separators) > 1:\n",
    "        # Si se detecta un espacio entre columnas, divide la imagen en dos columnas\n",
    "        middle_separator = (column_separators[0] + column_separators[-1]) // 2\n",
    "        left_column = img[:, :middle_separator]\n",
    "        right_column = img[:, middle_separator:]\n",
    "        return left_column, right_column\n",
    "    else:\n",
    "        return img, None\n",
    "\n",
    "def ocr_full(img):\n",
    "    # Realiza la detección de texto y OCR\n",
    "    result = ocr.ocr(img, cls=True)\n",
    "    columna_izquierda = []\n",
    "    columna_derecha = []\n",
    "\n",
    "    if result is None or len(result) == 0:\n",
    "        print(\"No text detected in the image.\")\n",
    "    return \"\", \"\"\n",
    "\n",
    "    # Itera sobre las cajas detectadas\n",
    "    for res in result:\n",
    "        if res is None:\n",
    "            continue\n",
    "        for line in res:\n",
    "            box = line[0]\n",
    "\n",
    "            # Extrae los valores X de las coordenadas de la caja, asegurándose de que sean numéricos\n",
    "            x_coords = [point[0] for point in box]\n",
    "\n",
    "            # Calcula el centro de la caja en el eje X\n",
    "            if len(x_coords) > 0:\n",
    "                centro_x = np.mean(x_coords)\n",
    "                # Clasifica la caja en columna izquierda o derecha según el centro X\n",
    "                img_width = img.shape[1]\n",
    "                if centro_x < img_width / 2:\n",
    "                    columna_izquierda.append(line[1][0])  # El texto está en line[1][0]\n",
    "                else:\n",
    "                    columna_derecha.append(line[1][0])\n",
    "\n",
    "    # Unir las palabras de cada columna\n",
    "    texto_columna_izquierda = ' '.join(columna_izquierda)\n",
    "    texto_columna_derecha = ' '.join(columna_derecha)\n",
    "    return texto_columna_izquierda, texto_columna_derecha\n",
    "    \n",
    "def ocr_column(column_image):\n",
    "    if column_image is not None:\n",
    "        # Convierte la imagen de columna de numpy a PIL Image\n",
    "        pil_img = Image.fromarray(column_image)\n",
    "\n",
    "        # Aplica OCR en la columna\n",
    "        result = ocr.ocr(np.array(pil_img))\n",
    "        # Extrae el texto\n",
    "        if result[0] is not None:\n",
    "            text = \" \".join([line[1][0] for line in result[0]])\n",
    "        else:\n",
    "            text = \"\"\n",
    "        return text\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def ocr_column_doctr(column_image):\n",
    "    if column_image is not None:\n",
    "        column_image = np.ascontiguousarray(column_image)\n",
    "        \n",
    "        # If the image is grayscale (2D), convert it to 3-channel\n",
    "        if len(column_image.shape) == 2:\n",
    "            column_image = np.stack((column_image,) * 3, axis=-1)\n",
    "        elif len(column_image.shape) == 3 and column_image.shape[2] == 1:\n",
    "            column_image = np.repeat(column_image, 3, axis=2)\n",
    "\n",
    "        # Use the OCR model\n",
    "        model = ocr_predictor(pretrained=True)\n",
    "        result = model([column_image])\n",
    "        if len(result.pages[0].blocks) == 0:\n",
    "            return \"\"\n",
    "        text = result.render()\n",
    "        return text\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def ocr_column_surya(column_image):\n",
    "    text = \"\"\n",
    "    if column_image is not None:\n",
    "        column_image = Image.fromarray(column_image)\n",
    "        langs = [\"es\"]\n",
    "        null_stream = io.StringIO()\n",
    "        try:\n",
    "            with redirect_stdout(null_stream), redirect_stderr(null_stream):\n",
    "                from surya.recognition import RecognitionPredictor\n",
    "                from surya.detection import DetectionPredictor\n",
    "                # Create predictor instances\n",
    "                detection_predictor = DetectionPredictor()\n",
    "                recognition_predictor = RecognitionPredictor()\n",
    "                \n",
    "                # Run OCR using the new API\n",
    "                predictions = recognition_predictor([column_image], [langs], detection_predictor)\n",
    "                \n",
    "                if predictions and predictions[0] is not None:\n",
    "                    # Access text_lines as an attribute, not using get()\n",
    "                    text_lines = predictions[0].text_lines\n",
    "                    text = \" \".join([line.text for line in text_lines])\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            gc.collect()\n",
    "    return text\n",
    "\n",
    "\n",
    "def ocr_column_olmo(img_array):\n",
    "    # Build the prompt, using document metadata\n",
    "    #anchor_text = get_anchor_text(\"./dev/pdf/9284.pdf\", 1, pdf_engine=\"pdfreport\", target_length=4000)\n",
    "    anchor_text = \"\"\n",
    "    prompt = build_finetuning_prompt(anchor_text)\n",
    "    pil_img = Image.fromarray(img_array) if img_array.shape[-1] == 3 else Image.fromarray(img_array, mode=\"L\")\n",
    "\n",
    "    # Guardar la imagen en memoria como PNG\n",
    "    buffer = BytesIO()\n",
    "    pil_img.save(buffer, format=\"PNG\")\n",
    "\n",
    "    # Convertir a base64\n",
    "    img_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
    "\n",
    "    # Build the full prompt\n",
    "    messages = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": prompt},\n",
    "                        {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{img_base64}\"}},\n",
    "                    ],\n",
    "                }\n",
    "            ]\n",
    "    \n",
    "    # Apply the chat template and processor\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    main_image = Image.open(BytesIO(base64.b64decode(img_base64)))\n",
    "    \n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=[main_image],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = {key: value.to(device) for (key, value) in inputs.items()}\n",
    "    \n",
    "    # Generate the output\n",
    "    output = model.generate(\n",
    "                **inputs,\n",
    "                temperature=0.8,\n",
    "                max_new_tokens=1000,\n",
    "                num_return_sequences=1,\n",
    "                do_sample=True,\n",
    "            )\n",
    "    \n",
    "    # Decode the output\n",
    "    prompt_length = inputs[\"input_ids\"].shape[1]\n",
    "    new_tokens = output[:, prompt_length:]\n",
    "    data = processor.tokenizer.batch_decode(\n",
    "        new_tokens, skip_special_tokens=True\n",
    "    )\n",
    "    json_str = data[0]\n",
    "    json_obj = json.loads(json_str)\n",
    "    text = json_obj[\"natural_text\"]\n",
    "    return text\n",
    "\n",
    "\n",
    "def ocr_image(img, page_num, file_path, ocr_path):\n",
    "    full_text = \"\"\n",
    "    paddl_text = \"\"\n",
    "    doctr_text = \"\"\n",
    "    surya_text = \"\"\n",
    "    olmo_text = \"\"\n",
    "\n",
    "    text_list = []\n",
    "    result = {key: None for key in keys}\n",
    "    result[\"file_path\"] = file_path\n",
    "    result[\"page_num\"] = page_num\n",
    "\n",
    "    img = np.array(img)\n",
    "    # Detectar columnas\n",
    "    paddl_text = postprocess(ocr_column(img))\n",
    "    result[\"paddle_CER\"], result[\"paddle_WER\"] = verify_text(paddl_text, ocr_path)\n",
    "    doctr_text = postprocess(ocr_column_doctr(img))\n",
    "    result[\"doctr_CER\"], result[\"doctr_WER\"] = verify_text(doctr_text, ocr_path)\n",
    "    surya_text = postprocess(ocr_column_surya(img))\n",
    "    result[\"surya_CER\"], result[\"surya_WER\"] = verify_text(surya_text, ocr_path)\n",
    "    olmo_text = postprocess(ocr_column_olmo(img))\n",
    "    result[\"olmo_CER\"], result[\"olmo_WER\"] = verify_text(olmo_text, ocr_path)\n",
    "    if paddl_text != \"\":\n",
    "        text_list.append(paddl_text)\n",
    "    if doctr_text != \"\":\n",
    "        text_list.append(doctr_text)\n",
    "    if surya_text != \"\":\n",
    "        text_list.append(surya_text)\n",
    "    #if olmo_text != \"\":\n",
    "    #    text_list.append(olmo_text)\n",
    "   \n",
    "    if len(text_list) > 0:\n",
    "        text = \"\\n\".join([f\"\\nText\\n{s}\" for s in text_list])\n",
    "        text_fixed = postprocess(fix_text(text))\n",
    "        result[\"fixed_CER\"], result[\"fixed_WER\"] = verify_text(text_fixed, ocr_path)\n",
    "        \n",
    "        if debug_mode:\n",
    "            full_text = f\"[page {page_num+1}]\\n[OCRs]\\n{text}\\n[Result]\\n{text_fixed}\\n\\n\"\n",
    "        else:\n",
    "            #full_text = f\"[page {page_num+1}]\\n{text_fixed}\\n\\n\"\n",
    "            full_text = text_fixed\n",
    "    else:\n",
    "       #full_text = f\"[page {page_num+1}]\\n[Página sin texto]\\n\\n\"\n",
    "        full_text = f\"[Página sin texto]\\n\\n\"\n",
    "    return full_text, result\n",
    "\n",
    "def read_pdf(file_path, ocr_path):\n",
    "    text = \"\"\n",
    "    result = {key: None for key in keys}\n",
    "    results = []\n",
    "\n",
    "    doc = fitz.open(file_path)\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        pix = page.get_pixmap()\n",
    "        img_data = pix.tobytes(\"png\")\n",
    "        img = Image.open(BytesIO(img_data))\n",
    "        # Convertir la imagen a escala de grises (mejora el OCR)\n",
    "        img = img.convert('L')\n",
    "        img.save(f\"page_{page_num}.png\")\n",
    "        print(f'Performing OCR in page {page_num+1}/{len(doc)}')\n",
    "        ocr_text = \"\"\n",
    "        ocr_text, result = ocr_image(img, page_num, file_path, ocr_path)\n",
    "        results.append(result)\n",
    "        text += ocr_text\n",
    "    print(results)\n",
    "    return text\n",
    "\n",
    "def verify_text(text, ocr_path):\n",
    "\n",
    "    def format_with_comma(number):\n",
    "        return f\"{number:.2f}\".replace('.', ',')\n",
    "       \n",
    "    def normalize_text(input_text):\n",
    "        # Eliminar última línea si contiene \"biblioteca nacional\"\n",
    "        input_text = re.sub(r'\\s*biblioteca\\s+nacional\\s+de\\s+españa\\s*$', '', input_text, flags=re.IGNORECASE)\n",
    "        lines = input_text.split('\\n')\n",
    "\n",
    "        if lines and len(lines) > 0:\n",
    "            # Usar expresión regular para eliminar cualquier secuencia de dígitos\n",
    "            lines[0] = re.sub(r'\\d+', '', lines[0])\n",
    "            # Only access the second line if it exists\n",
    "            if len(lines) > 1:\n",
    "                lines[1] = re.sub(r'\\d+', '', lines[1])\n",
    "       \n",
    "        # Volver a unir las líneas\n",
    "        text_clean = '\\n'.join(lines)\n",
    "\n",
    "        # Eliminar caracteres especiales: \", », «, -, —\n",
    "        text_clean = text_clean.replace('\"', '')\n",
    "        text_clean = text_clean.replace('»', '')\n",
    "        text_clean = text_clean.replace('«', '')\n",
    "        text_clean = text_clean.replace('-', '')\n",
    "        text_clean = text_clean.replace('—', '')\n",
    "        \n",
    "        # Normalizar espacios\n",
    "        # 2. Eliminar espacios al inicio y final de cada línea\n",
    "        text_clean = '\\n'.join(line.strip() for line in text_clean.split('\\n'))\n",
    "        # 3. Eliminar líneas en blanco consecutivas\n",
    "        #text_clean = re.sub(r'\\n\\s*\\n', '\\n\\n', text_clean)\n",
    "        text_clean = re.sub(r'\\n', ' ', text_clean)\n",
    "        # 1. Reemplazar múltiples espacios con uno solo\n",
    "        text_clean = re.sub(r' +', ' ', text_clean)\n",
    "        text_clean = re.sub(r' +\\.', '.', text_clean)\n",
    "       \n",
    "        return text_clean\n",
    "    \n",
    "    # Comprobar si existe el fichero\n",
    "    if not os.path.exists(ocr_path):\n",
    "        print(f\"Error: El archivo {ocr_path} no existe.\")\n",
    "        return\n",
    "    \n",
    "    # Cargar el contenido del fichero en la variable Ref\n",
    "    try:\n",
    "        with open(ocr_path, 'r', encoding='utf-8') as file:\n",
    "            Ref = file.read()\n",
    "        \n",
    "        # Normalizar ambos textos\n",
    "        normalized_text = normalize_text(text)\n",
    "        normalized_ref = normalize_text(Ref)\n",
    "        #print(\"[text]\\n\", normalized_text)\n",
    "        #print(\"[Ref]\\n\", normalized_ref)\n",
    "        \n",
    "        # Calcular métricas\n",
    "        cer = fastwer.score_sent(normalized_text, normalized_ref, char_level=True)\n",
    "        wer = fastwer.score_sent(normalized_text, normalized_ref)\n",
    "        \n",
    "        #print(f'CER:{format_with_comma(cer)} - WER:{format_with_comma(wer)}')\n",
    "        \n",
    "        # Devolver los valores para posible uso posterior\n",
    "        return cer, wer\n",
    "    except Exception as e:\n",
    "        print(f\"Error al leer el archivo: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "def postprocess(text):\n",
    "    pattern = r'(\\w+)-\\s+(\\w+)'\n",
    "    # Reemplaza con las dos partes unidas\n",
    "    text = re.sub(pattern, r'\\1\\2', text)\n",
    "    text = re.sub(r\"\\s(vio)\\s\", r\" vió \", text)\n",
    "   \n",
    "    return text\n",
    "\n",
    "def process_directory(directory, directory_ocr):\n",
    "    for file in os.scandir(directory):\n",
    "        if file.is_file() and file.name.lower().endswith('.pdf'):\n",
    "            file_path = file.path\n",
    "            base_name = os.path.splitext(file.name)[0]\n",
    "            txt_file_name = base_name + '_.txt'\n",
    "            txt_file_path = os.path.join(directory, txt_file_name)\n",
    "            ocr_name = base_name + '.txt'\n",
    "            ocr_path = os.path.join(directory_ocr, ocr_name)\n",
    "            \n",
    "            if os.path.exists(txt_file_path):\n",
    "                if show_skip:\n",
    "                    print(f\"Skipping {file_path} - _.txt file already exists\")\n",
    "                continue\n",
    "            print(file_path)\n",
    "            text = read_pdf(file_path, ocr_path)\n",
    "            text = postprocess(text)\n",
    "            save_text_file(text, txt_file_path)\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            gc.collect()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7710018",
   "metadata": {},
   "source": [
    "## Algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d6ea88",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_directory(directory, directory_ocr)\n",
    "print('Process finished!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdd0750-4359-4bae-8289-4a466540ed3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import doctr\n",
    "import paddleocr\n",
    "print(doctr.__version__)\n",
    "print(paddleocr.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda9d041-e9f1-4772-ad25-615a1d96df30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render page 2 to an image\n",
    "image_base64 = render_pdf_to_base64png(\"./dev/pdf/9284.pdf\", 1, target_longest_image_dim=1024)\n",
    "\n",
    "# Build the prompt, using document metadata\n",
    "anchor_text = get_anchor_text(\"./dev/pdf/9284.pdf\", 1, pdf_engine=\"pdfreport\", target_length=4000)\n",
    "prompt = build_finetuning_prompt(anchor_text)\n",
    "print(prompt)\n",
    "# Build the full prompt\n",
    "messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{image_base64}\"}},\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "\n",
    "# Apply the chat template and processor\n",
    "text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "main_image = Image.open(BytesIO(base64.b64decode(image_base64)))\n",
    "\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=[main_image],\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = {key: value.to(device) for (key, value) in inputs.items()}\n",
    "\n",
    "# Generate the output\n",
    "output = model.generate(\n",
    "            **inputs,\n",
    "            temperature=0.8,\n",
    "            max_new_tokens=1000,\n",
    "            num_return_sequences=1,\n",
    "            do_sample=True,\n",
    "        )\n",
    "\n",
    "# Decode the output\n",
    "prompt_length = inputs[\"input_ids\"].shape[1]\n",
    "new_tokens = output[:, prompt_length:]\n",
    "text_output = processor.tokenizer.batch_decode(\n",
    "    new_tokens, skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(text_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "augur",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

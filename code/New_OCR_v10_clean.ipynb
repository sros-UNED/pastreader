{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ed39d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Memory status] 5494MiB / 32760MiB - 28064153600\n",
      "./dev/test2/5.pdf\n",
      "[surya] CER:5.41 - WER:13.71\n",
      "[surya] CER:10.42 - WER:26.29\n",
      "./dev/test2/32.pdf\n",
      "[surya] CER:26.63 - WER:35.36\n",
      "[surya] CER:5.06 - WER:15.47\n",
      "./dev/test2/55.pdf\n",
      "[surya] CER:21.31 - WER:54.38\n",
      "[surya] CER:10.44 - WER:36.25\n",
      "./dev/test2/11.pdf\n",
      "[surya] CER:25.06 - WER:48.99\n",
      "[surya] CER:7.83 - WER:26.85\n",
      "./dev/test2/26.pdf\n",
      "[surya] CER:53.37 - WER:58.74\n",
      "[surya] CER:35.79 - WER:62.94\n",
      "./dev/test2/77.pdf\n",
      "Error querying the remote LLM service.\n",
      "[surya] CER:100.00 - WER:100.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function SyncHttpxClientWrapper.__del__ at 0x75a38132af20>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tmp/anaconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 778, in __del__\n",
      "    def __del__(self) -> None:\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1342\u001b[0m\n\u001b[1;32m   1339\u001b[0m fix_with_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1341\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Memory status]\u001b[39m\u001b[38;5;124m\"\u001b[39m, check_mem())\n\u001b[0;32m-> 1342\u001b[0m process_directory(directory, directory_ocr, zoom)\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124molmocr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ocr_model:\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m(model)\n",
      "Cell \u001b[0;32mIn[22], line 1002\u001b[0m, in \u001b[0;36mprocess_directory\u001b[0;34m(directory, directory_ocr, zoom)\u001b[0m\n\u001b[1;32m   1000\u001b[0m         Ref \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1001\u001b[0m         doc \u001b[38;5;241m=\u001b[39m fitz\u001b[38;5;241m.\u001b[39mopen(file_path)\n\u001b[0;32m-> 1002\u001b[0m         read_pdf(doc, Ref, ocr_path2)\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1004\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError al leer el archivo: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[22], line 949\u001b[0m, in \u001b[0;36mread_pdf\u001b[0;34m(doc, Ref, ocr_path2)\u001b[0m\n\u001b[1;32m    947\u001b[0m verify_text(engine, text, Ref)\n\u001b[1;32m    948\u001b[0m text_list\u001b[38;5;241m.\u001b[39mappend(text)\n\u001b[0;32m--> 949\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mglobals\u001b[39m()[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mocr_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mengine\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m](Image\u001b[38;5;241m.\u001b[39mfromarray(img))\n\u001b[1;32m    950\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mglobals\u001b[39m()[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpostprocess_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mengine\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m](text)\n\u001b[1;32m    951\u001b[0m verify_text(engine, text, Ref)\n",
      "Cell \u001b[0;32mIn[22], line 498\u001b[0m, in \u001b[0;36mocr_surya\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m img \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m redirect_stdout(null_stream), redirect_stderr(null_stream):\n\u001b[0;32m--> 498\u001b[0m         predictions \u001b[38;5;241m=\u001b[39m recognition_predictor([img], [langs], detection_predictor)\n\u001b[1;32m    499\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m predictions \u001b[38;5;129;01mand\u001b[39;00m predictions[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m             \u001b[38;5;66;03m# Access text_lines as an attribute\u001b[39;00m\n\u001b[1;32m    501\u001b[0m             text_lines \u001b[38;5;241m=\u001b[39m predictions[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext_lines\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/surya/recognition/__init__.py:75\u001b[0m, in \u001b[0;36mRecognitionPredictor.__call__\u001b[0;34m(self, images, langs, det_predictor, detection_batch_size, recognition_batch_size, highres_images, bboxes, polygons, sort_lines)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(images) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(polygons), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to pass in one list of polygons for each image\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     68\u001b[0m     flat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslice_bboxes(\n\u001b[1;32m     69\u001b[0m         images,\n\u001b[1;32m     70\u001b[0m         langs,\n\u001b[1;32m     71\u001b[0m         bboxes\u001b[38;5;241m=\u001b[39mbboxes,\n\u001b[1;32m     72\u001b[0m         polygons\u001b[38;5;241m=\u001b[39mpolygons\n\u001b[1;32m     73\u001b[0m     )\n\u001b[0;32m---> 75\u001b[0m rec_predictions, confidence_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_recognition(\n\u001b[1;32m     76\u001b[0m     flat[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mslices\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     77\u001b[0m     flat[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlangs\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     78\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mrecognition_batch_size\n\u001b[1;32m     79\u001b[0m )\n\u001b[1;32m     81\u001b[0m predictions_by_image \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     82\u001b[0m slice_start \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/surya/recognition/__init__.py:278\u001b[0m, in \u001b[0;36mRecognitionPredictor.batch_recognition\u001b[0;34m(self, images, languages, batch_size)\u001b[0m\n\u001b[1;32m    276\u001b[0m is_prefill \u001b[38;5;241m=\u001b[39m token_count \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;66;03m# TODO: add attention mask\u001b[39;00m\n\u001b[0;32m--> 278\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdecoder(\n\u001b[1;32m    279\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39mbatch_decoder_input,\n\u001b[1;32m    280\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_text_hidden_states,\n\u001b[1;32m    281\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mdecoder_position_ids,\n\u001b[1;32m    282\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    283\u001b[0m     prefill\u001b[38;5;241m=\u001b[39mis_prefill\n\u001b[1;32m    284\u001b[0m )\n\u001b[1;32m    286\u001b[0m decoder_position_ids \u001b[38;5;241m=\u001b[39m decoder_position_ids[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    287\u001b[0m logits \u001b[38;5;241m=\u001b[39m return_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# Ignore batch padding\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/surya/recognition/model/decoder.py:74\u001b[0m, in \u001b[0;36mSuryaOCRDecoder.forward\u001b[0;34m(self, input_ids, cache_position, attention_mask, encoder_hidden_states, encoder_attention_mask, use_cache, prefill, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     65\u001b[0m     input_ids: Optional[torch\u001b[38;5;241m.\u001b[39mLongTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m     73\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple, OCRModelOutput]:\n\u001b[0;32m---> 74\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m     75\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m     76\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m     77\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m     78\u001b[0m         encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[1;32m     79\u001b[0m         encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[1;32m     80\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m     81\u001b[0m         output_hidden_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     82\u001b[0m         return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     83\u001b[0m         prefill\u001b[38;5;241m=\u001b[39mprefill,\n\u001b[1;32m     84\u001b[0m     )\n\u001b[1;32m     86\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     87\u001b[0m     all_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/surya/common/adetr/decoder.py:579\u001b[0m, in \u001b[0;36mSuryaADETRDecoderModel.forward\u001b[0;34m(self, input_ids, input_boxes_counts, inputs_embeds, position_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, cache_position, use_cache, output_hidden_states, return_dict, prefill)\u001b[0m\n\u001b[1;32m    575\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    576\u001b[0m             residual_block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, hidden_states, position_ids, causal_mask, encoder_hidden_states, encoder_attention_mask, cache_position, use_cache\n\u001b[1;32m    577\u001b[0m         )\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 579\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m residual_block(hidden_states, position_ids, causal_mask, encoder_hidden_states, encoder_attention_mask, cache_position, use_cache)\n\u001b[1;32m    581\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_norm(hidden_states)\n\u001b[1;32m    583\u001b[0m \u001b[38;5;66;03m# add hidden states from the last decoder layer\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/surya/common/adetr/decoder.py:379\u001b[0m, in \u001b[0;36mSuryaADETRDecoderLayer.forward\u001b[0;34m(self, activations, position_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, cache_position, use_cache)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    369\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    370\u001b[0m         activations: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    376\u001b[0m         use_cache: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    377\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdouble_residual_flow:\n\u001b[0;32m--> 379\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdouble_res_forward(\n\u001b[1;32m    380\u001b[0m             activations, position_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, cache_position, use_cache\n\u001b[1;32m    381\u001b[0m         )\n\u001b[1;32m    383\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m activations\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_attn_block \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m         \u001b[38;5;66;03m# Do cross-attention on encoder outputs\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/surya/common/adetr/decoder.py:440\u001b[0m, in \u001b[0;36mSuryaADETRDecoderLayer.double_res_forward\u001b[0;34m(self, activations, position_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, cache_position, use_cache)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    438\u001b[0m     residual \u001b[38;5;241m=\u001b[39m cross_attn_output\n\u001b[0;32m--> 440\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannel_pre_norm(residual)\n\u001b[1;32m    441\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_block(hidden_states)\n\u001b[1;32m    443\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m residual\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/surya/common/adetr/decoder.py:37\u001b[0m, in \u001b[0;36mSuryaADETRDecoderRMSNorm.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 37\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_norm(x\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# Llama does x.to(float16) * w whilst SuryaADETRDecoder is (x * w).to(float16)\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# See https://github.com/huggingface/transformers/pull/29402\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     output \u001b[38;5;241m=\u001b[39m output \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mfloat())\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/surya/common/adetr/decoder.py:30\u001b[0m, in \u001b[0;36mSuryaADETRDecoderRMSNorm._norm\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_norm\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 30\u001b[0m     variance \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# Add clipping to prevent division by zero\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     variance \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(variance, \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tqdm, os, math\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "import re, gc, io, torch, fastwer, cv2, fitz, json, subprocess\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from paddleocr import PaddleOCR\n",
    "from doctr.models import ocr_predictor\n",
    "from together import Together\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import base64, urllib.request\n",
    "import google.generativeai as genai\n",
    "from olmocr.data.renderpdf import render_pdf_to_base64png\n",
    "from olmocr.prompts import build_finetuning_prompt\n",
    "from olmocr.prompts.anchor import get_anchor_text\n",
    "from transformers import AutoProcessor, Qwen2VLForConditionalGeneration, AutoModelForImageTextToText\n",
    "from contextlib import redirect_stdout, redirect_stderr\n",
    "from surya.recognition import RecognitionPredictor\n",
    "from surya.detection import DetectionPredictor\n",
    "from surya.layout import LayoutPredictor\n",
    "from openai import AzureOpenAI\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "from skimage import filters, exposure, util\n",
    "\n",
    "langs = [\"es\"]\n",
    "ocr_model = [\"surya\"] # \"paddle\", \"doctr\", \"gemini\", \"surya\", \"olmocr\"\n",
    "fix_model = [] # \"gpt-4o\", \"gpt-4o-mini\", \"gemini\", \"gemini2\"\n",
    "pos_model = [] # \"gpt-4o\", \"gpt-4o-mini\", \"gemini\", \"gemini2\"\n",
    "pos_model_surya = [\"gpt-4o\"] # \"gpt-4o\", \"gpt-4o-mini\", \"gemini\", \"gemini2\"\n",
    "\n",
    "\n",
    "### UTIL FUNCTIONS #################################################################\n",
    "\n",
    "try:\n",
    "    # Intentamos importar pynvml para GPUs NVIDIA\n",
    "    import pynvml\n",
    "    has_gpu = True\n",
    "except ImportError:\n",
    "    # Si no está disponible, usamos psutil para memoria del sistema\n",
    "    import psutil\n",
    "    has_gpu = False\n",
    "\n",
    "def check_mem():\n",
    "    mem = \"\"\n",
    "    memoria_disponible = \"\"\n",
    "    \n",
    "    result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, text=True)\n",
    "    output = result.stdout\n",
    "    memory_pattern = re.search(r'(\\d+)MiB\\s+/\\s+(\\d+)MiB', output)\n",
    "    if memory_pattern:\n",
    "        used_mem = memory_pattern.group(1)\n",
    "        total_mem = memory_pattern.group(2)\n",
    "        mem = f\"{used_mem}MiB / {total_mem}MiB\"\n",
    "\n",
    "    if has_gpu:\n",
    "        pynvml.nvmlInit()\n",
    "        handle = pynvml.nvmlDeviceGetHandleByIndex(0)  # Usando primera GPU\n",
    "        info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "        memoria_disponible = info.free\n",
    "        pynvml.nvmlShutdown()\n",
    "    else:\n",
    "        memoria_disponible = psutil.virtual_memory().available\n",
    "    if memoria_disponible != \"\":\n",
    "        mem += f\" - {memoria_disponible}\"\n",
    "\n",
    "    return mem\n",
    "\n",
    "def save_text_file(text, output_file):\n",
    "    if text != \"\":\n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            file.write(text)\n",
    "\n",
    "def verify_text(engine, text, Ref):\n",
    "    def normalize_text(input_text):\n",
    "        input_text = re.sub(r'===END===', '', input_text)\n",
    "        input_text = re.sub(r'\\s*biblioteca\\s+nacional\\s+de\\s+españa\\s*$', '', input_text, flags=re.IGNORECASE)\n",
    "        lines = input_text.split('\\n')\n",
    "        if lines and len(lines) > 0:\n",
    "            lines[0] = re.sub(r'\\d+', '', lines[0])\n",
    "            if len(lines) > 1:\n",
    "                lines[1] = re.sub(r'\\d+', '', lines[1])\n",
    "        text_clean = '\\n'.join(lines)\n",
    "        text_clean = re.sub(r'[\"„©¶»«—]', '', text_clean)\n",
    "        text_clean = '\\n'.join(line.strip() for line in text_clean.split('\\n'))\n",
    "        text_clean = re.sub(r'\\n', ' ', text_clean)\n",
    "        text_clean = re.sub(r' +', ' ', text_clean)\n",
    "        text_clean = re.sub(r' +\\.', '.', text_clean)\n",
    "        return text_clean\n",
    "\n",
    "    normalized_text = normalize_text(text)\n",
    "    normalized_ref = normalize_text(Ref)\n",
    "    pattern = r'(\\w+)[\\-\\u00AD\\u2010\\u2011]+\\s+(\\w+)'\n",
    "    normalized_ref = re.sub(pattern, r'\\1\\2', normalized_ref)\n",
    "    #print(\"[text]\\n\", normalized_text)\n",
    "    #print(\"[Ref]\\n\", normalized_ref)\n",
    "    \n",
    "    cer = fastwer.score_sent(normalized_text, normalized_ref, char_level=True)\n",
    "    wer = fastwer.score_sent(normalized_text, normalized_ref)\n",
    "    print(f'[{engine}] CER:{cer:.2f} - WER:{wer:.2f}')\n",
    "    return cer, wer\n",
    "\n",
    "def postprocess(text):\n",
    "    pattern = r'(\\w+)[\\-\\u00AD\\u2010\\u2011]+\\s+(\\w+)'\n",
    "    # Reemplaza con las dos partes unidas\n",
    "    text = re.sub(pattern, r'\\1\\2', text)\n",
    "    #text = re.sub(r\"(\\w+)-\\s*\\n\\s*\", r\"\\1\", text)\n",
    "    text = re.sub(r\"\\s(vio)\\s\", r\" vió \", text)\n",
    "    text = re.sub(r\"\\s(fue)\\s\", r\" fué \", text)\n",
    "    if debug:\n",
    "        print(text)\n",
    "    return text\n",
    "\n",
    "def postprocess_surya(text):\n",
    "    def replace_accents(match):\n",
    "        return replacement_dict[match.group(0)]\n",
    "\n",
    "    pattern = r'à|è|ì|ò|ù|À|È|Ì|Ò|Ù'\n",
    "    replacement_dict = {\n",
    "        'à': 'á', 'è': 'é', 'ì': 'í', 'ò': 'ó', 'ù': 'ú',\n",
    "        'À': 'Á', 'È': 'É', 'Ì': 'Í', 'Ò': 'Ó', 'Ù': 'Ú'\n",
    "    }\n",
    "\n",
    "    #text = re.sub(pattern, replace_accents, text)\n",
    "    text = re.sub(r'\\s*biblioteca\\s+nacional\\s+de\\s+espa[ñn]a\\s*$', '', text, flags=re.IGNORECASE)\n",
    "    pattern = r'(\\w+)-\\s+(\\w+)' \n",
    "    text = re.sub(pattern, r'\\1\\2', text)\n",
    "    text = re.sub(r\"[\\u00A9\\u24B8]\", \"\", text) # ©\n",
    "    \n",
    "    #text = re.sub(r\"(\\w+)\\s*-\\s*\\n\\s*(\\w+)\", r\"\\1\\2\", text)\n",
    "    text = re.sub(r\"\\s(vio)\\s\", r\" vió \", text)\n",
    "    text = re.sub(r\"\\s(fue)\\s\", r\" fué \", text)\n",
    "    if len(pos_model_surya)>0:\n",
    "        text = pos_remote(text, pos_model_surya[0])\n",
    "    text = re.sub(r'===END===', '', text)\n",
    "    return postprocess(text)\n",
    "\n",
    "def postprocess_doctr(text):\n",
    "    return postprocess(text)\n",
    "\n",
    "def postprocess_paddle(text):\n",
    "    return postprocess(text)\n",
    "\n",
    "def postprocess_gemini(text):\n",
    "    return postprocess(text)\n",
    "   \n",
    "def postprocess_olmocr(text):\n",
    "    return postprocess(text)\n",
    "   \n",
    "# --- Filtros individuales ---\n",
    "\n",
    "def reescalar(img):\n",
    "    return cv2.resize(img, None, fx=upscale, fy=upscale, interpolation=cv2.INTER_CUBIC)\n",
    "    \n",
    "def limpiar_fondo(img):\n",
    "    fondo = cv2.medianBlur(img, 21)\n",
    "    resta = cv2.absdiff(img, fondo)\n",
    "    norm = cv2.normalize(resta, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    return norm\n",
    "\n",
    "def brillo_contraste(img, alpha=1.0, beta=30):\n",
    "    return cv2.convertScaleAbs(img, alpha=alpha, beta=beta)\n",
    "\n",
    "def suavizado(img):\n",
    "    return cv2.medianBlur(img, 3)\n",
    "\n",
    "def enfoque(img):\n",
    "    kernel = np.array([[0, -1, 0],\n",
    "                       [-1, 5, -1],\n",
    "                       [0, -1, 0]])\n",
    "    return cv2.filter2D(img, -1, kernel)\n",
    "\n",
    "def clahe(img):\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    return clahe.apply(img)\n",
    "\n",
    "def binar_adaptativa(img):\n",
    "    return cv2.adaptiveThreshold(img, 255,\n",
    "                                 cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                 cv2.THRESH_BINARY_INV,\n",
    "                                 15, 7)\n",
    "\n",
    "def otsu_inv(img):\n",
    "    _, binaria = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "    return binaria\n",
    "\n",
    "def otsu_normal(img):\n",
    "    _, binaria = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    return binaria\n",
    "\n",
    "def dilatacion(img):\n",
    "    kernel = np.ones((1, 1), np.uint8)\n",
    "    return cv2.dilate(img, kernel, iterations=1)\n",
    "\n",
    "def preprocess(img_orig, filtros_activados):\n",
    "    try:\n",
    "        img = img_orig.copy()\n",
    "\n",
    "        filtros = {\n",
    "            1: reescalar,\n",
    "            2: limpiar_fondo,\n",
    "            3: brillo_contraste,\n",
    "            4: suavizado,\n",
    "            5: enfoque,\n",
    "            6: clahe,\n",
    "            7: binar_adaptativa,\n",
    "            8: otsu_inv,\n",
    "            9: otsu_normal,\n",
    "            10: dilatacion\n",
    "        }\n",
    "\n",
    "        # 1. Asegurar que sea uint8 y escala de grises (igual que opción 1)\n",
    "        if len(img_orig.shape) == 3 and img_orig.shape[2] == 3:\n",
    "            img_gray = cv2.cvtColor(img_orig, cv2.COLOR_BGR2GRAY)\n",
    "        else:\n",
    "            img_gray = img_orig.copy()\n",
    "\n",
    "        if img_gray.dtype != np.uint8:\n",
    "           img_gray = cv2.normalize(img_gray, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "\n",
    "        for id_filtro in filtros_activados:\n",
    "            if id_filtro not in filtros:\n",
    "                print(f\"Filtro {id_filtro} no está definido.\")\n",
    "                continue\n",
    "\n",
    "            # Aplicar filtro\n",
    "            filtro_func = filtros[id_filtro]\n",
    "\n",
    "            # Filtros con parámetros especiales\n",
    "            if id_filtro == 3:\n",
    "                img = filtro_func(img, alpha=1.2, beta=30)\n",
    "            else:\n",
    "                img = filtro_func(img)\n",
    "\n",
    "        return img\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error en preprocess: {str(e)}\")\n",
    "        return img_orig\n",
    "\n",
    "def probar_combinaciones_top10(img_original, Ref):\n",
    "    combinaciones = [\n",
    "        [1, 3, 5, 8],\n",
    "        [1, 3, 6, 8],\n",
    "        [1, 4, 6, 8],\n",
    "        [1, 2, 6, 8],\n",
    "        [1, 2, 4, 8],\n",
    "        [1, 6, 8],\n",
    "    ]\n",
    "    page_num = 1\n",
    "\n",
    "    for i, combo in enumerate(combinaciones):\n",
    "        try:\n",
    "            print(f\"\\n▶️ Probando combinación #{i+1}: {combo}\")\n",
    "            img_proc = preprocess(img_original, combo)\n",
    "            cv2.imwrite(f\"page_{page_num}_.png\", img_proc)\n",
    "\n",
    "            # Realizar OCR\n",
    "            layout = process_layout(page_num)\n",
    "            engine = \"olmocr\"\n",
    "            text = process_images(engine, layout[\"cropped_images\"])\n",
    "            text = globals()[f\"postprocess_{engine}\"](text)\n",
    "            verify_text(engine, text, Ref)\n",
    "            text = globals()[f\"ocr_{engine}\"](Image.fromarray(img_proc)) # layout[\"combined_image\"]\n",
    "            text = globals()[f\"postprocess_{engine}\"](text)\n",
    "            verify_text(engine, text, Ref)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error en combinación {i+1} {combo}: {e}\")\n",
    "            continue\n",
    "\n",
    "import pytesseract\n",
    "\n",
    "def analizar_img(path_imagen):\n",
    "    imagen = cv2.imread(path_imagen)\n",
    "\n",
    "    if imagen is None:\n",
    "        return \"ERROR,No se pudo cargar la imagen\"\n",
    "\n",
    "    gris = cv2.cvtColor(imagen, cv2.COLOR_BGR2GRAY)\n",
    "    alto, ancho = gris.shape\n",
    "\n",
    "    # 1. Brillo y contraste\n",
    "    brillo = np.mean(gris)\n",
    "    contraste = np.std(gris)\n",
    "\n",
    "    # 2. Detección de columnas (basado en contornos grandes)\n",
    "    _, binaria = cv2.threshold(gris, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    bin_inv = 255 - binaria\n",
    "    contornos, _ = cv2.findContours(bin_inv, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    columnas_detectadas = sum(1 for c in contornos if cv2.boundingRect(c)[2] > ancho // 3)\n",
    "\n",
    "    # 3. Espaciado entre líneas y palabras\n",
    "    datos_ocr = pytesseract.image_to_data(gris, output_type=pytesseract.Output.DICT)\n",
    "    y_coords = [datos_ocr['top'][i] for i in range(len(datos_ocr['text'])) if int(datos_ocr['conf'][i]) > 0 and datos_ocr['text'][i].strip() != '']\n",
    "    if len(y_coords) > 1:\n",
    "        y_coords.sort()\n",
    "        espaciado_lineas = np.mean(np.diff(y_coords))\n",
    "    else:\n",
    "        espaciado_lineas = 0\n",
    "\n",
    "    # 4. Inclinación del texto (skew)\n",
    "    coords = np.column_stack(np.where(gris < 128))\n",
    "    if len(coords) == 0:\n",
    "        skew = 0\n",
    "    else:\n",
    "        angulo = cv2.minAreaRect(coords)[-1]\n",
    "        if angulo < -45:\n",
    "            angulo = -(90 + angulo)\n",
    "        else:\n",
    "            angulo = -angulo\n",
    "        skew = angulo\n",
    "\n",
    "    # 5. Tipo de fondo (ruido/patrones)\n",
    "    fondo_var = np.var(gris)\n",
    "    fondo_ruido = fondo_var > 500  # Umbral empírico\n",
    "\n",
    "    # 6. Color del texto/fondo\n",
    "    blancos = np.sum(binaria == 255)\n",
    "    negros = np.sum(binaria == 0)\n",
    "    texto_oscuro = negros > blancos  # True si el texto es negro\n",
    "\n",
    "    # 7. Artefactos de compresión (bordes artificiales)\n",
    "    bloques = cv2.Laplacian(gris, cv2.CV_64F).var()\n",
    "    artefactos_compresion = bloques < 50  # Umbral empírico\n",
    "\n",
    "    # 8. Márgenes (bordes blancos)\n",
    "    top_margin = np.mean(gris[0:10, :])\n",
    "    bottom_margin = np.mean(gris[-10:, :])\n",
    "    left_margin = np.mean(gris[:, 0:10])\n",
    "    right_margin = np.mean(gris[:, -10:])\n",
    "    margen_blanco = all(m > 240 for m in [top_margin, bottom_margin, left_margin, right_margin])\n",
    "\n",
    "    # 9. Tamaño promedio del texto\n",
    "    alturas = [datos_ocr['height'][i] for i in range(len(datos_ocr['height'])) if int(datos_ocr['conf'][i]) > 0]\n",
    "    if alturas:\n",
    "        altura_prom_texto = int(np.mean(alturas))\n",
    "    else:\n",
    "        altura_prom_texto = 0\n",
    "    tam_texto_relativo = altura_prom_texto / alto\n",
    "\n",
    "    # 10. Densidad de bordes\n",
    "    bordes = cv2.Canny(gris, 100, 200)\n",
    "    densidad_bordes = np.sum(bordes > 0) / (gris.shape[0] * gris.shape[1])\n",
    "\n",
    "    # 11. Ruido estimado\n",
    "    ruido_estimado = np.var(util.random_noise(gris/255.0, mode='gaussian') - gris/255.0)\n",
    "\n",
    "    # 12. Proporción de texto\n",
    "    texto_negro = np.sum(binaria == 0)\n",
    "    texto_fondo_ratio = texto_negro / binaria.size\n",
    "\n",
    "    # 13. Componentes conectados\n",
    "    num_componentes, _ = cv2.connectedComponents(binaria)\n",
    "\n",
    "    # 14. Presencia de líneas/tablas\n",
    "    horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (40, 1))\n",
    "    vertical_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 40))\n",
    "    detect_horizontal = cv2.morphologyEx(binaria, cv2.MORPH_OPEN, horizontal_kernel)\n",
    "    detect_vertical = cv2.morphologyEx(binaria, cv2.MORPH_OPEN, vertical_kernel)\n",
    "    lineas_detectadas = (np.sum(detect_horizontal == 0) + np.sum(detect_vertical == 0)) > 500  # Arbitrario\n",
    "\n",
    "    # 15. Generar sugerencias\n",
    "    sugerencias = []\n",
    "\n",
    "    if brillo < 80:\n",
    "        sugerencias.append(\"mejorar_brillo\")\n",
    "    if contraste < 30:\n",
    "        sugerencias.append(\"mejorar_contraste\")\n",
    "    if densidad_bordes < 0.01:\n",
    "        sugerencias.append(\"aplicar_enfoque\")\n",
    "    if ruido_estimado > 0.01:\n",
    "        sugerencias.append(\"suavizar\")\n",
    "    if skew > 2:\n",
    "        sugerencias.append(\"corregir_inclinacion\")\n",
    "    if altura_prom_texto < 15:\n",
    "        sugerencias.append(\"reescalar\")\n",
    "    if not texto_oscuro:\n",
    "        sugerencias.append(\"invertir_colores\")\n",
    "    if fondo_ruido:\n",
    "        sugerencias.append(\"limpiar_fondo\")\n",
    "    if margen_blanco:\n",
    "        sugerencias.append(\"recortar_margenes\")\n",
    "    if artefactos_compresion:\n",
    "        sugerencias.append(\"reducir_artefactos\")\n",
    "\n",
    "    # 16. Formato de salida CSV\n",
    "    fila_csv = (\n",
    "        f\"{brillo:.2f},{contraste:.2f},{skew:.2f},{altura_prom_texto},\"\n",
    "        f\"{tam_texto_relativo:.4f},{espaciado_lineas:.2f},{columnas_detectadas},{densidad_bordes:.4f},\"\n",
    "        f\"{ruido_estimado:.4f},{texto_fondo_ratio:.4f},{num_componentes},{int(texto_oscuro)},\"\n",
    "        f\"{int(fondo_ruido)},{int(margen_blanco)},{int(artefactos_compresion)},{int(lineas_detectadas)},\"\n",
    "        f\"{'|'.join(sugerencias)}\"\n",
    "    )\n",
    "    return fila_csv\n",
    "\n",
    "def fix_text_with_image_remote_gemini(text, page_num):\n",
    "    image_path = f\"page_{page_num}_.png\"\n",
    "    img = Image.open(image_path)\n",
    "    \n",
    "    # Enviar solicitud al modelo\n",
    "    respuesta = gmodel.generate_content([text, img])\n",
    "    \n",
    "    # Extraer texto de la respuesta\n",
    "    corrected_text = respuesta.text\n",
    "    \n",
    "    return corrected_text\n",
    "\n",
    "def fix_text_with_image_remote(text, page_num):\n",
    "    image_path = f\"page_{page_num}_.png\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        img = image_file.read()\n",
    "    base64_image = base64.b64encode(img).decode('utf-8')\n",
    "    \n",
    "    # Crear el mensaje con contenido mixto (texto e imagen)\n",
    "    prompt = set_prompt_with_image(text)\n",
    "    \n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=deployment,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": prompt},\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\": f\"data:image/png;base64,{base64_image}\"\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=4000,\n",
    "            temperature=0,\n",
    "            top_p=0.95,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            stop=None,\n",
    "            stream=False\n",
    "        )\n",
    "        data = json.loads(completion.model_dump_json())\n",
    "        corrected_text = data['choices'][0]['message']['content']\n",
    "        parts = corrected_text.split('===END===')\n",
    "        if len(parts) > 1:\n",
    "            return parts[0].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying the remote LLM service: {e}\")\n",
    "        torch.cuda.empty_cache()\n",
    "    return corrected_text\n",
    "\n",
    "def fix_text_remote_gemini(prompt):\n",
    "    corrected_text = \"\"\n",
    "    try:\n",
    "        if \"gemini2\" in pos_model:\n",
    "            respuesta = gmodel2.generate_content(prompt)\n",
    "        else:\n",
    "            respuesta = gmodel.generate_content(prompt)\n",
    "        corrected_text = respuesta.text\n",
    "    except Exception as e:\n",
    "        print(f\"[fix_text_remote_gemini] {e}\")\n",
    "        torch.cuda.empty_cache()\n",
    "    return corrected_text\n",
    "\n",
    "def fix_text_remote(prompt, deploy):\n",
    "    corrected_text = \"\"\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=deploy ,\n",
    "            messages= [\n",
    "            {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": prompt\n",
    "            }],\n",
    "            max_tokens=4000,\n",
    "            temperature=0,\n",
    "            top_p=0.95,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            stop=None,\n",
    "            stream=False\n",
    "        )\n",
    "        data = json.loads(completion.model_dump_json())\n",
    "        corrected_text = data['choices'][0]['message']['content']\n",
    "        parts = corrected_text.split('===END===')\n",
    "        if len(parts) > 1:\n",
    "            return parts[0].strip()\n",
    "    except:\n",
    "        print(\"Error querying the remote LLM service.\")\n",
    "        torch.cuda.empty_cache()\n",
    "    return corrected_text\n",
    "\n",
    "def pos_prompt(text):\n",
    "    prompt = \"\"\"\n",
    "    You are an expert in historical Spanish linguistics and orthography. Your task is to carefully review the following text written in Old Spanish (from the 16th to 19th centuries). You must analyze the text sentence by sentence, verifying the following aspects:\n",
    "    1. Meaning & Coherence:\n",
    "    - Does each sentence make logical sense in the context of Old Spanish?\n",
    "    - Identify and highlight any phrases that seem unclear, ambiguous, or grammatically incorrect.\n",
    "    2. Punctuation & Special Characters:\n",
    "    - Ensure that question marks (¿...?) and exclamation marks (¡...!) are correctly opened and closed according to Spanish grammar rules.\n",
    "    - Check for missing or misplaced punctuation marks that may affect readability.\n",
    "    3. Orthography Consistency (Old Spanish Standards):\n",
    "    - Verify whether the spelling is consistent with Old Spanish conventions of its time.\n",
    "    - Identify anachronisms or incorrect modernized spellings.\n",
    "    - Ensure that words are spelled as they would have been during the era the text belongs to.\n",
    "    4. Suggestions & Corrections:\n",
    "    If errors are found, provide a corrected version while preserving the historical style.\n",
    "    Explain any changes made, especially if modern Spanish rules conflict with historical usage.\n",
    "    \n",
    "    Instructions:\n",
    "    - Maintain the original tone and structure of the text.\n",
    "    - Do not modernize the language.\n",
    "    - Provide only the corrected text, without any additional commentary.\n",
    "    - Do not add any new information or explanations.\n",
    "    - Focus on fixing spelling and obvious OCR mistakes by comparing with the original image.\n",
    "    - End your response with '===END===' on a new line.\n",
    "    \n",
    "    Input Example:\n",
    "    \"¿Quántos daños no causarán los criados con su olvido ó mala inteligencia de los recados que reciben?\"\n",
    "    \n",
    "    Expected Output:\n",
    "    - Sentence makes sense.\n",
    "    - Proper use of question marks (¿...? is correctly opened and closed).\n",
    "    - \"Quántos\" should be corrected to \"¿Cuántos?\" (based on orthographic conventions of the period).\n",
    "    - \"ó\" with an accent is valid in Old Spanish when avoiding vowel collision.\n",
    "    \"\"\"\n",
    "    prompt += f\"\\nHere is the text:\\n\\n{text}\"\n",
    "    return prompt\n",
    "\n",
    "def set_prompt(text):\n",
    "    prompt = (\n",
    "        \"You are an expert in text correction and OCR error fixing. Your task is to combine and correct several OCR outputs of the same text. \"\n",
    "        f\"Here are the texts:\\n\\n{text}\"\n",
    "        \"\\n\\nInstructions:\\n\"\n",
    "        \"1. Combine the texts, correcting any OCR errors.\\n\"\n",
    "        \"2. Provide only the corrected text, without any additional commentary.\\n\"\n",
    "        \"3. Maintain the original structure and formatting.\\n\"\n",
    "        \"4. Do not add any new information or explanations.\\n\"\n",
    "        \"5. Join any words that have been separated by a hyphen at the end of a line. If there're blank spaces after the hyphen, remove them so the two parts of the word get joined correctly.\\n\"\n",
    "        \"6. The text is written using archaic Spanish spelling.\\n\"\n",
    "        \"7. Maintain all diacritical marks, old-fashioned spellings, and historical punctuation, such as the use of 'fué' instead of 'fue', 'dió' instead of 'dio', 'ví' instead of 'vi', 'á' instead of 'a' in prepositions. Do not replace older words or grammatical structures with modern equivalents.\\n\"\n",
    "        \"8. Ensure that all words retain their original diacritics, such as accents (é, á, ó), tildes (ñ), and umlauts (ü), without alteration.\\n\"\n",
    "        \"9. Focus on fixing spelling and obvious OCR mistakes.\\n\"\n",
    "        \"10. End your response with '===END===' on a new line.\\n\\n\"\n",
    "        \"Corrected text:\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def set_prompt_with_image(text):\n",
    "    prompt = (\n",
    "        \"You are an expert in text correction and OCR error fixing. Your task is to combine and correct several OCR outputs of the same text. \"\n",
    "        \"I'm providing both the OCR outputs and the original image of the document. \"\n",
    "        f\"Here are the OCR texts:\\n\\n{text}\"\n",
    "        \"\\n\\nInstructions:\\n\"\n",
    "        \"1. First, look at the image of the original document to understand the correct text.\\n\"\n",
    "        \"2. Compare the OCR outputs with what you see in the image and create the most accurate version.\\n\"\n",
    "        \"3. Combine the texts, correcting any OCR errors based on what's visible in the image.\\n\"\n",
    "        \"4. When the OCR outputs differ, refer to the image to determine the correct text.\\n\"\n",
    "        \"5. Provide only the corrected text, without any additional commentary.\\n\"\n",
    "        \"6. Maintain the original structure and formatting.\\n\"\n",
    "        \"7. Do not add any new information or explanations.\\n\"\n",
    "        \"8. Join any words that have been separated by a hyphen at the end of a line. If there're blank spaces after the hyphen, remove them so the two parts of the word get joined correctly.\\n\"\n",
    "        \"9. The text is written using archaic Spanish spelling.\\n\"\n",
    "        \"10. Maintain all diacritical marks, old-fashioned spellings, and historical punctuation, such as the use of 'fué' instead of 'fue', 'dió' instead of 'dio', 'ví' instead of 'vi', 'á' instead of 'a' in prepositions. Do not replace older words or grammatical structures with modern equivalents.\\n\"\n",
    "        \"11. Ensure that all words retain their original diacritics, such as accents (é, á, ó), tildes (ñ), and umlauts (ü), without alteration.\\n\"\n",
    "        \"12. Focus on fixing spelling and obvious OCR mistakes by comparing with the original image.\\n\"\n",
    "        \"13. End your response with '===END===' on a new line.\\n\\n\"\n",
    "        \"Corrected text:\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def fix_text(text):\n",
    "    prompt = set_prompt(text)\n",
    "    corrected_text = \"\"\n",
    "    #print(prompt)\n",
    "    if \"gemini\" in fix_model:\n",
    "        corrected_text = fix_text_remote_gemini(prompt)\n",
    "    elif \"gpt-4o\" in fix_model:\n",
    "        corrected_text = fix_text_remote(prompt, deployment)\n",
    "    else:\n",
    "        corrected_text = fix_text_remote(prompt, deployment_mini)\n",
    "    return corrected_text\n",
    "\n",
    "def fix_text_with_images(text, page_num):\n",
    "    prompt = set_prompt_with_image(text)\n",
    "    corrected_text = \"\"\n",
    "    if \"gemini\" in fix_model:\n",
    "        corrected_text = fix_text_with_image_remote_gemini(prompt, page_num)\n",
    "    else:\n",
    "        corrected_text = fix_text_with_image_remote(prompt, page_num)\n",
    "    return corrected_text\n",
    "\n",
    "def pos_remote(text, pmodel=\"\"):\n",
    "    prompt = pos_prompt(text)\n",
    "    corrected_text = \"\"\n",
    "    #print(prompt)\n",
    "    if pmodel == \"gemini\":\n",
    "        corrected_text = fix_text_remote_gemini(prompt)\n",
    "    elif pmodel == \"gpt-4o\":\n",
    "        corrected_text = fix_text_remote(prompt, deployment)\n",
    "    elif pmodel == \"gpt-4o-mini\":\n",
    "        corrected_text = fix_text_remote(prompt, deployment_mini)\n",
    "    #print('[corrected_text]', corrected_text)\n",
    "    return corrected_text\n",
    "\n",
    "### API MODELS #################################################################\n",
    "\n",
    "if \"gemini\" in ocr_model or \"gemini\" in fix_model:\n",
    "    gemini_api_key = \"\"\n",
    "    genai.configure(api_key=gemini_api_key)\n",
    "    gmodel = genai.GenerativeModel('gemini-2.0-flash-thinking-exp-01-21')\n",
    "    if \"gemini2\" in pos_model:\n",
    "        gmodel2 = genai.GenerativeModel('gemini-2.5-pro-exp-03-25')\n",
    " \n",
    "if \"gpt-4o\" in ocr_model or \"gpt-4o\" in fix_model or \"gpt-4o\" in pos_model or \"gpt-4o\" in pos_model_surya:\n",
    "    endpoint = os.getenv(\"ENDPOINT_URL\", \"https://open-ia-service.openai.azure.com/\")\n",
    "    deployment = os.getenv(\"DEPLOYMENT_NAME\", \"gpt-4o\")\n",
    "    deployment_mini = os.getenv(\"DEPLOYMENT_NAME\", \"gpt-4o-mini\")\n",
    "    #AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "    AZURE_OPENAI_API_KEY = os.environ.get(\"AZURE_OPENAI_API_KEY\")\n",
    "    client = AzureOpenAI(\n",
    "        api_key = AZURE_OPENAI_API_KEY,\n",
    "        api_version = \"2024-05-01-preview\",\n",
    "        azure_endpoint = endpoint \n",
    "    )\n",
    "\n",
    "if \"r1\" in ocr_model or \"r1\" in fix_model:\n",
    "    client = Together(api_key = \"xxx\")\n",
    "### LOCAL MODELS #################################################################\n",
    "\n",
    "def ocr_gemini(img):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        prompt = \"\"\"\n",
    "        Perform OCR (Optical Character Recognition) on this image.\n",
    "        Extract ALL visible text without modernizing or modifying Old Spanish.\n",
    "        Correct spelling and punctuation while preserving the original language and format.\n",
    "        Respond ONLY with the extracted text, without additional comments.\n",
    "        \"\"\"\n",
    "        respuesta = gmodel.generate_content([prompt, img])\n",
    "        text = respuesta.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error[ocr_gemini]: {str(e)}\")\n",
    "    return text\n",
    "\n",
    "paddle_ocr = PaddleOCR(show_log=False, use_angle_cls=True, lang='es', use_gpu=False)\n",
    "def ocr_paddle(img):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        if img is not None:\n",
    "            result = paddle_ocr.ocr(img)\n",
    "            if result[0] is not None:\n",
    "                text = \" \".join([line[1][0] for line in result[0]])\n",
    "    except Exception as e:\n",
    "        print(f\"Error[ocr_paddle]: {str(e)}\")\n",
    "    return text\n",
    "\n",
    "def ocr_doctr(img):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        if img is not None:\n",
    "            model = ocr_predictor(pretrained=True)\n",
    "            result = model([img])\n",
    "            if len(result.pages[0].blocks) == 0:\n",
    "                return \"\"\n",
    "            text = result.render()\n",
    "    except Exception as e:\n",
    "        print(f\"Error[ocr_doctr]: {str(e)}\")\n",
    "    return text\n",
    "\n",
    "os.environ[\"TQDM_DISABLE\"] = \"1\"\n",
    "null_stream = io.StringIO()\n",
    "with redirect_stdout(null_stream), redirect_stderr(null_stream):\n",
    "    detection_predictor = DetectionPredictor()\n",
    "    recognition_predictor = RecognitionPredictor()\n",
    "    layout_predictor = LayoutPredictor()\n",
    "def ocr_surya(img):\n",
    "    text = \"\"\n",
    "    null_stream = io.StringIO()\n",
    "    try:\n",
    "        if img is not None:\n",
    "            with redirect_stdout(null_stream), redirect_stderr(null_stream):\n",
    "                predictions = recognition_predictor([img], [langs], detection_predictor)\n",
    "                if predictions and predictions[0] is not None:\n",
    "                    # Access text_lines as an attribute\n",
    "                    text_lines = predictions[0].text_lines\n",
    "                    text = \" \".join([line.text for line in text_lines])\n",
    "    except Exception as e:\n",
    "        tmp = f\"Error[ocr_surya]: {str(e)}\"\n",
    "        if 'fillPoly' not in str(e):\n",
    "            print(tmp)\n",
    "    return text\n",
    "\n",
    "if \"olmocr\" in ocr_model:\n",
    "    #processor = AutoProcessor.from_pretrained(\"reducto/RolmOCR\")\n",
    "    #model = AutoModelForImageTextToText.from_pretrained(\"reducto/RolmOCR\")\n",
    "    #model = model.half()\n",
    "\n",
    "    processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
    "    model = Qwen2VLForConditionalGeneration.from_pretrained(\"allenai/olmOCR-7B-0225-preview\", torch_dtype=torch.bfloat16).eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "def ocr_Rolmocr(img):\n",
    "    in_text = \"\"\n",
    "    out_text = \"\"\n",
    "    torch.cuda.empty_cache()  # Liberar memoria no utilizada\n",
    "     \n",
    "    try:\n",
    "        # Verificar tamaño de la imagen\n",
    "        width, height = img.size\n",
    "        min_size = 32  # Un poco más grande que el factor requerido (28)\n",
    "        # Si la imagen es demasiado pequeña, redimensionarla manteniendo la relación de aspecto\n",
    "        if width < min_size or height < min_size:\n",
    "            # Calcular la escala necesaria\n",
    "            scale = max(min_size / width, min_size / height)\n",
    "            new_width = int(width * scale)\n",
    "            new_height = int(height * scale)\n",
    "            img = img.resize((new_width, new_height), Image.LANCZOS)\n",
    "\n",
    "        area = width * height\n",
    "        max_area = 1800000\n",
    "        if area > max_area:\n",
    "            # Ajustar el factor de escala según el tamaño\n",
    "            scale = math.sqrt(max_area / area)\n",
    "            \n",
    "            new_width = int(width * scale)\n",
    "            new_height = int(height * scale)\n",
    "            #print(f\"Redimensionando a {new_width}x{new_height} (factor: {scale:.2f})\")\n",
    "            img = img.resize((new_width, new_height), Image.LANCZOS)\n",
    "\n",
    "        # Build the prompt, using document metadata\n",
    "        #anchor_text = get_anchor_text(\"./dev/pdf/9284.pdf\", 1, pdf_engine=\"pdfreport\", target_length=4000)\n",
    "        anchor_text = \"\"\n",
    "        prompt = build_finetuning_prompt(anchor_text)\n",
    "        #pil_img = Image.fromarray(img) if img.shape[-1] == 3 else Image.fromarray(img, mode=\"L\")\n",
    "        buffer = BytesIO()\n",
    "        #pil_img.save(buffer, format=\"PNG\")\n",
    "        img.save(buffer, format=\"PNG\")\n",
    "        img_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
    "        #img_base64 = base64.b64encode(img).decode('utf-8')\n",
    "    \n",
    "        # Build the full prompt\n",
    "        messages = [{\"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\"type\": \"text\", \"text\": prompt},\n",
    "                            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{img_base64}\"}},\n",
    "                        ],}]\n",
    "        \n",
    "        # Apply the chat template and processor\n",
    "        in_text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        \n",
    "        inputs = processor(\n",
    "            text=[in_text],\n",
    "            images=[img],\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        inputs = {key: value.to(device) for (key, value) in inputs.items()}\n",
    "        del img_base64, buffer\n",
    "        torch.cuda.empty_cache()  # Liberar memoria no utilizada\n",
    "         \n",
    "        # Generate the output\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            temperature=0.3,       # Aumentar ligeramente para más variedad\n",
    "            top_p=0.7,             # Ampliar el conjunto de tokens considerados\n",
    "            max_new_tokens=2048,\n",
    "            num_return_sequences=1,\n",
    "            do_sample=True,\n",
    "            use_cache=True  # Asegurar que el caché está activado\n",
    "        )\n",
    "        # Decode the output\n",
    "        prompt_length = inputs[\"input_ids\"].shape[1]\n",
    "        new_tokens = output[:, prompt_length:]\n",
    "        data = processor.tokenizer.batch_decode(\n",
    "            new_tokens, skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        del inputs, output, new_tokens\n",
    "        torch.cuda.empty_cache()\n",
    "        print('[olmo_output]\\n', data)\n",
    "        out_text = data[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Error[ocr_olmo]: {str(e)}\")\n",
    "    return out_text\n",
    "\n",
    "def remove_repetitions(text, max_repetitions=5):\n",
    "    # Dividir en líneas\n",
    "    lines = text.split('\\\\n')\n",
    "    \n",
    "    # Detectar y eliminar líneas repetidas\n",
    "    unique_lines = []\n",
    "    repeat_count = 0\n",
    "    prev_line = None\n",
    "    \n",
    "    for line in lines:\n",
    "        if line == prev_line:\n",
    "            repeat_count += 1\n",
    "            if repeat_count < max_repetitions:\n",
    "                unique_lines.append(line)\n",
    "        else:\n",
    "            repeat_count = 0\n",
    "            unique_lines.append(line)\n",
    "            prev_line = line\n",
    "    \n",
    "    return '\\\\n'.join(unique_lines)\n",
    "\n",
    "def repair_json_attempt(json_str):\n",
    "    # Buscar patrones de repetición en todo el string\n",
    "    # Esta es una implementación básica que busca repeticiones obvias\n",
    "    lines = json_str.split('\\\\n')\n",
    "    unique_lines = []\n",
    "    prev_line = None\n",
    "    repeat_count = 0\n",
    "    max_repetitions = 2\n",
    "    \n",
    "    for line in lines:\n",
    "        if line == prev_line:\n",
    "            repeat_count += 1\n",
    "            if repeat_count < max_repetitions:\n",
    "                unique_lines.append(line)\n",
    "        else:\n",
    "            repeat_count = 0\n",
    "            unique_lines.append(line)\n",
    "            prev_line = line\n",
    "    \n",
    "    # Reconstruir el string\n",
    "    repaired_str = '\\\\n'.join(unique_lines)\n",
    "    \n",
    "    # Asegurarse de que termina correctamente\n",
    "    if not repaired_str.endswith('\"}'):\n",
    "        repaired_str = repaired_str + '\"}'\n",
    "    \n",
    "    # Intentar parsear de nuevo\n",
    "    try:\n",
    "        json.loads(repaired_str)\n",
    "        return repaired_str\n",
    "    except:\n",
    "        # Si aún falla, devolver una estructura JSON de error\n",
    "        return None\n",
    "\n",
    "def ocr_olmocr(img):\n",
    "    in_text = \"\"\n",
    "    out_text = \"\"\n",
    "    torch.cuda.empty_cache()  # Liberar memoria no utilizada\n",
    "     \n",
    "    try:\n",
    "        # Verificar tamaño de la imagen\n",
    "        width, height = img.size\n",
    "        min_size = 32  # Un poco más grande que el factor requerido (28)\n",
    "        # Si la imagen es demasiado pequeña, redimensionarla manteniendo la relación de aspecto\n",
    "        if width < min_size or height < min_size:\n",
    "            # Calcular la escala necesaria\n",
    "            scale = max(min_size / width, min_size / height)\n",
    "            new_width = int(width * scale)\n",
    "            new_height = int(height * scale)\n",
    "            img = img.resize((new_width, new_height), Image.LANCZOS)\n",
    "\n",
    "        area = width * height\n",
    "        max_area = 1800000\n",
    "        if area > max_area:\n",
    "            # Ajustar el factor de escala según el tamaño\n",
    "            scale = math.sqrt(max_area / area)\n",
    "            \n",
    "            new_width = int(width * scale)\n",
    "            new_height = int(height * scale)\n",
    "            #print(f\"Redimensionando a {new_width}x{new_height} (factor: {scale:.2f})\")\n",
    "            img = img.resize((new_width, new_height), Image.LANCZOS)\n",
    "\n",
    "        # Build the prompt, using document metadata\n",
    "        #anchor_text = get_anchor_text(\"./dev/pdf/9284.pdf\", 1, pdf_engine=\"pdfreport\", target_length=4000)\n",
    "        anchor_text = \"\"\n",
    "        prompt = build_finetuning_prompt(anchor_text)\n",
    "        #pil_img = Image.fromarray(img) if img.shape[-1] == 3 else Image.fromarray(img, mode=\"L\")\n",
    "        buffer = BytesIO()\n",
    "        #pil_img.save(buffer, format=\"PNG\")\n",
    "        img.save(buffer, format=\"PNG\")\n",
    "        img_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
    "        #img_base64 = base64.b64encode(img).decode('utf-8')\n",
    "    \n",
    "        # Build the full prompt\n",
    "        messages = [{\"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\"type\": \"text\", \"text\": prompt},\n",
    "                            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{img_base64}\"}},\n",
    "                        ],}]\n",
    "        \n",
    "        # Apply the chat template and processor\n",
    "        in_text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        \n",
    "        inputs = processor(\n",
    "            text=[in_text],\n",
    "            images=[img],\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        inputs = {key: value.to(device) for (key, value) in inputs.items()}\n",
    "        del img_base64, buffer\n",
    "        torch.cuda.empty_cache()  # Liberar memoria no utilizada\n",
    "         \n",
    "        # Generate the output\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            temperature=0.7,       # Aumentar ligeramente para más variedad\n",
    "            top_p=0.7,             # Ampliar el conjunto de tokens considerados\n",
    "            max_new_tokens=2048,\n",
    "            num_return_sequences=1,\n",
    "            do_sample=True,\n",
    "            use_cache=True  # Asegurar que el caché está activado\n",
    "       )\n",
    "        # Decode the output\n",
    "        prompt_length = inputs[\"input_ids\"].shape[1]\n",
    "        new_tokens = output[:, prompt_length:]\n",
    "        data = processor.tokenizer.batch_decode(\n",
    "            new_tokens, skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        del inputs, output, new_tokens\n",
    "        torch.cuda.empty_cache()\n",
    "        #print('[olmo_output]\\n', data)\n",
    "        json_str = data[0]\n",
    "        cleaned_text = \"\"\n",
    "        try:\n",
    "            # Intentar parsear el JSON primero\n",
    "            json_obj = json.loads(json_str)\n",
    "            \n",
    "            # Si existe el campo natural_text, limpiarlo de repeticiones\n",
    "            if \"natural_text\" in json_obj and json_obj[\"natural_text\"] is not None:\n",
    "                out_text = remove_repetitions(json_obj[\"natural_text\"], 1)\n",
    "            \n",
    "        except json.JSONDecodeError:\n",
    "            # Si no se puede parsear, intentar reparar el JSON antes\n",
    "            cleaned_text = repair_json_attempt(json_str)\n",
    "            if cleaned_text is not None:\n",
    "                out_text = cleaned_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error[ocr_olmo]: {str(e)}\")\n",
    "    return out_text\n",
    "\n",
    "### PROCESS FLOW #################################################################\n",
    "\n",
    "def process_images(engine, images):\n",
    "    text_list = []\n",
    "    full_text = \"\"\n",
    "    \n",
    "    ocr_function_name = f\"ocr_{engine}\"\n",
    "    if ocr_function_name not in globals() or not images:\n",
    "        return \"\"\n",
    "    \n",
    "    # Procesar cada imagen recortada con el motor OCR especificado\n",
    "    for cropped_img in images:\n",
    "        cropped_array = np.array(cropped_img)\n",
    "        try:\n",
    "            # Seleccionar formato correcto según el motor\n",
    "            if engine in [\"paddle\", \"doctr\"]:\n",
    "                text = globals()[ocr_function_name](cropped_array)\n",
    "            else:\n",
    "                text = globals()[ocr_function_name](cropped_img)\n",
    "            # Procesar el texto según su tipo\n",
    "            if isinstance(text, dict) and 'text' in text:\n",
    "                processed_text = text['text'].strip()\n",
    "            elif isinstance(text, str):\n",
    "                processed_text = text.strip()\n",
    "            else:\n",
    "                processed_text = str(text).strip()\n",
    "\n",
    "            if processed_text:\n",
    "                text_list.append(processed_text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error en OCR {engine}: {str(e)}\")\n",
    "    \n",
    "    # Unir los textos\n",
    "    if text_list:\n",
    "        full_text = \"\\n\\n\".join(text_list)\n",
    "    return full_text\n",
    "\n",
    "def show_img(img, title=\"\"):\n",
    "    plt.imshow(img, cmap='gray')  # Especificamos escala de grises\n",
    "    plt.title(title)\n",
    "    plt.axis('off')  # Oculta los ejes\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def process_layout(page_num):\n",
    "    img_path = f\"page_{page_num}_.png\"\n",
    "    \n",
    "    try:\n",
    "        # Cargar imagen\n",
    "        img = Image.open(img_path)\n",
    "        img_np = np.array(img)\n",
    "        \n",
    "        # Predicción de layout\n",
    "        with redirect_stdout(io.StringIO()), redirect_stderr(io.StringIO()):\n",
    "            layout_predictions = layout_predictor([img])\n",
    "        \n",
    "        # Ordenar cajas por posición\n",
    "        boxes = sorted(layout_predictions[0].bboxes, key=lambda box: box.position)\n",
    "        \n",
    "        # Procesar cada caja individualmente\n",
    "        cropped_images = []\n",
    "        \n",
    "        for box in boxes:\n",
    "            x1, y1, x2, y2 = map(int, box.bbox)\n",
    "            cropped_array = img_np[y1:y2, x1:x2]\n",
    "            \n",
    "            if cropped_array.size == 0:\n",
    "                continue\n",
    "                \n",
    "            cropped_img = Image.fromarray(cropped_array)\n",
    "            cropped_images.append(cropped_img)\n",
    "        \n",
    "        # Detectar si hay columnas (más de una caja en la misma línea horizontal)\n",
    "        has_columns = False\n",
    "        if len(boxes) > 1:\n",
    "            # Ordenamos por coordenada y\n",
    "            y_sorted_boxes = sorted(boxes, key=lambda box: box.bbox[1])\n",
    "            \n",
    "            # Verificamos si hay cajas con superposición vertical significativa\n",
    "            for i in range(len(y_sorted_boxes) - 1):\n",
    "                box1 = y_sorted_boxes[i]\n",
    "                box2 = y_sorted_boxes[i + 1]\n",
    "                \n",
    "                # Si hay superposición vertical significativa entre cajas, consideramos que hay columnas\n",
    "                y1_1, y2_1 = box1.bbox[1], box1.bbox[3]\n",
    "                y1_2, y2_2 = box2.bbox[1], box2.bbox[3]\n",
    "                \n",
    "                overlap = min(y2_1, y2_2) - max(y1_1, y1_2)\n",
    "                if overlap > 0 and overlap / min(y2_1 - y1_1, y2_2 - y1_2) > 0.3:\n",
    "                    has_columns = True\n",
    "                    break\n",
    "        \n",
    "        # Si hay imágenes recortadas, combinarlas\n",
    "        combined_img = None\n",
    "        if cropped_images:\n",
    "            # Calcular la altura total y el ancho máximo\n",
    "            total_height = sum(img.height for img in cropped_images)\n",
    "            max_width = max(img.width for img in cropped_images)\n",
    "            \n",
    "            # Crear una nueva imagen con PIL\n",
    "            combined_img = Image.new('RGB', (max_width, total_height), (255, 255, 255))\n",
    "            \n",
    "            # Pegar cada fragmento en la imagen combinada\n",
    "            current_y = 0\n",
    "            for cropped_img in cropped_images:\n",
    "                combined_img.paste(cropped_img, (0, current_y))\n",
    "                current_y += cropped_img.height\n",
    "                if img_show:\n",
    "                    show_img(cropped_img, \"join\")\n",
    "            if img_show:\n",
    "                show_img(combined_img, \"combined\")\n",
    "        \n",
    "        return {\n",
    "            \"combined_image\": combined_img,\n",
    "            \"cropped_images\": cropped_images,\n",
    "            \"has_columns\": has_columns\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando layout de página {page_num}: {str(e)}\")\n",
    "        return {\n",
    "            \"combined_image\": None,\n",
    "            \"cropped_images\": [],\n",
    "            \"has_columns\": False\n",
    "        }\n",
    "\n",
    "def read_pdf(doc, Ref, ocr_path2):\n",
    "    text_list = []\n",
    "    try:\n",
    "        for page_num in range(len(doc)):\n",
    "    #        pix = page.get_pixmap(alpha=False)\n",
    "    #        pix.save(output_png_path)\n",
    "           \n",
    "            page = doc.load_page(page_num)\n",
    "            mat = fitz.Matrix(zoom, zoom)\n",
    "            pix = page.get_pixmap(matrix=mat, alpha=False)\n",
    "            #pix.save(f\"page_{page_num}_.png\")\n",
    "            img_data = pix.tobytes(\"png\")\n",
    "            img_pil = Image.open(BytesIO(img_data)).convert('L')\n",
    "            img_cv2 = np.array(img_pil)\n",
    "            img = preprocess(img_cv2, [1, 3, 5, 8])\n",
    "            cv2.imwrite(f\"page_{page_num}_.png\", img)\n",
    "                    #cv2.imwrite(f\"page_{page_num}_.png\", img_cv2)\n",
    "            #resultados = analizar_img(f\"page_{page_num}_.png\")\n",
    "            #print(resultados)\n",
    "                    #probar_combinaciones_top10(img_cv2, Ref)\n",
    "                    #continue\n",
    "\n",
    "            layout = process_layout(page_num)\n",
    "            #print(\"has_columns\", layout[\"has_columns\"])\n",
    "            if \"gemini\" in ocr_model:\n",
    "                engine = \"gemini\"\n",
    "                if layout[\"has_columns\"]:\n",
    "                    text = globals()[f\"ocr_{engine}\"](layout[\"combined_image\"])\n",
    "                else:\n",
    "                    text = globals()[f\"ocr_{engine}\"](Image.fromarray(img)) # layout[\"combined_image\"]\n",
    "                text = globals()[f\"postprocess_{engine}\"](text)\n",
    "                verify_text(engine, text, Ref)\n",
    "                text_list.append(text)\n",
    "            if \"paddle\" in ocr_model:\n",
    "                engine = \"paddle\"\n",
    "                text = process_images(engine, layout[\"cropped_images\"])\n",
    "                text = globals()[f\"postprocess_{engine}\"](text)\n",
    "                verify_text(engine, text, Ref)\n",
    "                text_list.append(text)\n",
    "            if \"doctr\" in ocr_model:\n",
    "                engine = \"doctr\"\n",
    "                text = process_images(engine, layout[\"cropped_images\"])\n",
    "                text = globals()[f\"postprocess_{engine}\"](text)\n",
    "                verify_text(engine, text, Ref)\n",
    "                text_list.append(text)\n",
    "            if \"surya\" in ocr_model:\n",
    "                engine = \"surya\"\n",
    "                text = process_images(engine, layout[\"cropped_images\"])\n",
    "                text = globals()[f\"postprocess_{engine}\"](text)\n",
    "                verify_text(engine, text, Ref)\n",
    "                text_list.append(text)\n",
    "                text = globals()[f\"ocr_{engine}\"](Image.fromarray(img))\n",
    "                text = globals()[f\"postprocess_{engine}\"](text)\n",
    "                verify_text(engine, text, Ref)\n",
    "                text_list.append(text)\n",
    "            if \"olmocr\" in ocr_model:\n",
    "                engine = \"olmocr\"\n",
    "                text = process_images(engine, layout[\"cropped_images\"])\n",
    "                text = globals()[f\"postprocess_{engine}\"](text)\n",
    "                verify_text(engine, text, Ref)\n",
    "                text_list.append(text)\n",
    "                text = globals()[f\"ocr_{engine}\"](Image.fromarray(img)) # layout[\"combined_image\"]\n",
    "                text = globals()[f\"postprocess_{engine}\"](text)\n",
    "                verify_text(engine, text, Ref)\n",
    "                text_list.append(text)\n",
    "\n",
    "            text = \"\\n\".join([f\"\\n<Text>\\n{s}\\n<\\\\Text>\\n\" for s in text_list])\n",
    "            save_text_file(text, ocr_path2)\n",
    "\n",
    "            if text_list and fix_model:\n",
    "                \n",
    "                if fix_with_images:\n",
    "                    text_fixed = postprocess(fix_text_with_images(text, page_num))\n",
    "                    verify_text(f\"{fix_model[0]} w/img\", text_fixed, Ref)\n",
    "                else:\n",
    "                    text_fixed = postprocess(fix_text(text))\n",
    "                    verify_text(fix_model[0], text_fixed, Ref)\n",
    "\n",
    "            if len(pos_model)>0:\n",
    "                text = pos_remote(text, pos_model[0])\n",
    "                verify_text(f\"pos_{pos_model[0]}\", text, Ref)\n",
    "            #ocr_image(img, Ref, page_num)\n",
    "    except Exception as e:\n",
    "        print(f\"Error[read_pdf]: {str(e)}\")\n",
    "\n",
    "def process_directory(directory, directory_ocr, zoom):\n",
    "    for file in os.scandir(directory):\n",
    "        if file.is_file() and file.name.lower().endswith('.pdf'):\n",
    "            file_path = file.path\n",
    "            base_name = os.path.splitext(file.name)[0]\n",
    "            ocr_name = base_name + '.txt'\n",
    "            ocr_path = os.path.join(directory_ocr, ocr_name)\n",
    "\n",
    "            ocr_name2 = base_name + '_.txt'\n",
    "            ocr_path2 = os.path.join(directory_ocr, ocr_name2)\n",
    "\n",
    "            print(file_path)\n",
    "            if not os.path.exists(ocr_path):\n",
    "                print(f\"Error: El archivo {ocr_path} no existe.\")\n",
    "                continue\n",
    "            try:\n",
    "                with open(ocr_path, 'r', encoding='utf-8') as file:\n",
    "                    Ref = file.read()\n",
    "                    doc = fitz.open(file_path)\n",
    "                    read_pdf(doc, Ref, ocr_path2)\n",
    "            except Exception as e:\n",
    "                print(f\"Error al leer el archivo: {str(e)}\")\n",
    "    \n",
    "directory = \"./dev/test2/\"\n",
    "directory_ocr = \"./dev/test2/\"\n",
    "\n",
    "zoom = 2\n",
    "upscale = 1.2  # (max_upscale)\n",
    "img_show = False\n",
    "debug = False\n",
    "fix_with_images = False\n",
    "\n",
    "print(\"[Memory status]\", check_mem())\n",
    "process_directory(directory, directory_ocr, zoom)\n",
    "\n",
    "if \"olmocr\" in ocr_model:\n",
    "    del(model)\n",
    "    del(processor)\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
